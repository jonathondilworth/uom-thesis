{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d66e0800",
   "metadata": {},
   "source": [
    "# Init'ing Encoders, etc (temporary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29596da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374536/374536 [00:03<00:00, 119117.74it/s]\n",
      "/tmp/ipykernel_3253638/2746723976.py:747: DeprecationWarning: SentenceTransformer.load(...) is deprecated, use SentenceTransformer(...) instead.\n",
      "  sbert_plm_encoder = SentenceTransformer.load(sbert_plm_hf_string)\n",
      "/tmp/ipykernel_3253638/2746723976.py:575: DeprecationWarning: SentenceTransformer.load(...) is deprecated, use SentenceTransformer(...) instead.\n",
      "  self._model = SentenceTransformer.load(model)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from pydantic import validate_call\n",
    "from rank_bm25 import BM25Okapi\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from argparse import ArgumentParser, Namespace\n",
    "from scipy.sparse import coo_matrix\n",
    "from hierarchy_transformers import HierarchyTransformer\n",
    "from OnT.OnT import OntologyTransformer\n",
    "\n",
    "from collections.abc import Generator, MutableMapping, Mapping, Sequence\n",
    "from typing import Union, NamedTuple, override, Any\n",
    "from functools import reduce\n",
    "import math\n",
    "\n",
    "from typing import override, Callable, Union, NamedTuple\n",
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import auc as sk_auc\n",
    "\n",
    "import statistics\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "from tqdm import tqdm\n",
    "import re # RegEx\n",
    "\n",
    "from typing import overload\n",
    "\n",
    "VERBOSE = False\n",
    "\n",
    "_regex_parens = re.compile(r\"\\s*\\([^)]*\\)\") # for parentheses removal (prevents leakage)\n",
    "\n",
    "def strip_parens(s: str) -> str:\n",
    "    return _regex_parens.sub(\"\", s)\n",
    "\n",
    "def load_json(file_path: Path) -> dict[str, str]:\n",
    "    with file_path.open('r', encoding='utf-8') as fp:\n",
    "        return json.load(fp)\n",
    "\n",
    "def save_json(file_path: Path, payload: dict | list, encoding: str = \"utf-8\", indentation: int = 4) -> None:\n",
    "    with open(file_path, \"w\", encoding=encoding) as fp:\n",
    "        json.dump(payload, fp, indent=indentation)\n",
    "\n",
    "def load_concepts_to_list(concepts_file_path: Path) -> list[str]:\n",
    "    return list(load_json(concepts_file_path).values())\n",
    "\n",
    "def naive_tokenise(seq: str) -> list[str]:\n",
    "    return seq.lower().split()\n",
    "\n",
    "# ~800k concepts, assuming 16 cores, 32 threads; \n",
    "# 800k / 32 = 25k (note: subprocesses != threads)\n",
    "def parallel_tokenise(seq_list: list[str], workers: int, chunksize: int = 25000) -> list[list[str]]:\n",
    "    with Pool(workers) as pool:\n",
    "        return list(pool.map(naive_tokenise, seq_list, chunksize=chunksize))\n",
    "\n",
    "\n",
    "def batch_euclidian_l2_distance(u: np.ndarray, vs: np.ndarray) -> np.ndarray:\n",
    "    return np.linalg.norm(u - vs, axis=1)\n",
    "\n",
    "\n",
    "def l2_norm(x: np.ndarray) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "    return np.sqrt(np.sum(x**2))\n",
    "\n",
    "\n",
    "def batch_l2_norm(x: np.ndarray) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "    return np.asarray(np.sqrt(np.sum(x**2, axis=1)))\n",
    "\n",
    "\n",
    "def inner_product(p_u: np.ndarray, p_v: np.ndarray) -> np.ndarray:\n",
    "    u = np.asarray(p_u, dtype=np.float32)\n",
    "    v = np.asarray(p_v, dtype=np.float32)\n",
    "    return np.inner(u, v)\n",
    "\n",
    "\n",
    "def batch_inner_product(p_u: np.ndarray, p_vs: np.ndarray) -> np.ndarray:\n",
    "    u = np.asarray(p_u, dtype=np.float32).ravel()\n",
    "    vs = np.asarray(p_vs, dtype=np.float32)\n",
    "    return vs.dot(u)\n",
    "\n",
    "\n",
    "def cosine_similarity(u, v, normalised=True):\n",
    "    u = np.asarray(u, dtype=np.float32)\n",
    "    v = np.asarray(v, dtype=np.float32)\n",
    "    return np.inner(u, v) if normalised else np.inner(u, v) / (l2_norm(u) * l2_norm(v))\n",
    "\n",
    "\n",
    "def batch_cosine_similarity(p_u, p_vs, normalised=True):\n",
    "    u  = np.asarray(p_u,  dtype=np.float32)\n",
    "    vs = np.asarray(p_vs, dtype=np.float32)\n",
    "    return batch_inner_product(u, vs) if normalised else batch_inner_product(u, vs) / (l2_norm(u) * batch_l2_norm(vs))\n",
    "\n",
    "\n",
    "def batch_poincare_distance_with_curv_k(u: np.ndarray, vs: np.ndarray, k: np.float64 | np.float32) -> np.float64 | np.float32:\n",
    "    u_norm_sqd = np.sum(u**2)\n",
    "    vs_norms_sqd = np.sum(vs**2, axis=1)\n",
    "    l2_dist_sqd = np.sum((u - vs)**2, axis=1)\n",
    "    offset = 1e-7 # tiny-offset: guard agaisnt division by zero & floating point arithmatic inaccuracies\n",
    "    arg = 1 + ((2 * k * l2_dist_sqd) / ((1 - (k * u_norm_sqd + offset)) * (1 - (k * vs_norms_sqd + offset)))) # acosh\n",
    "    arg = np.maximum(1.0, arg) # bounds check: domain of acosh is bound to [1, \\inf)\n",
    "    acosh_scaling = np.float64(1) / np.float64(np.sqrt(k)) # scaling factor: k\n",
    "    return (acosh_scaling * np.arccosh(arg, dtype=np.float64)) # 1 / sqrt(k) * acosh(arg)\n",
    "\n",
    "\n",
    "def batch_poincare_dist_with_adaptive_curv_k(u: np.ndarray, vs:np.ndarray, model: HierarchyTransformer | OntologyTransformer, **kwargs):\n",
    "    if isinstance(model, HierarchyTransformer):\n",
    "        k = np.float64(model.get_circum_poincareball(model.embed_dim).c)\n",
    "    elif isinstance(model, OntologyTransformer):\n",
    "        hierarchy_model = model.hit_model\n",
    "        hierarchy_poincare_ball = hierarchy_model.get_circum_poincareball(hierarchy_model.embed_dim)\n",
    "        k = np.float64(hierarchy_poincare_ball.c)    \n",
    "    else:\n",
    "        raise Exception(\"Hyperbolic distance should only be only calculated in B^n or H^n\")\n",
    "    return np.asarray(batch_poincare_distance_with_curv_k(u, vs, k))\n",
    "\n",
    "# additional metrics\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def subsumption_score_hit(hit_transformer: HierarchyTransformer, child_emb: np.ndarray | torch.Tensor, parent_emd: np.ndarray | torch.Tensor, centri_weight: float = 1.0):\n",
    "    child_emb_t = torch.Tensor(child_emb)\n",
    "    parent_emb_t = torch.Tensor(parent_emd)\n",
    "    dists = hit_transformer.manifold.dist(child_emb_t, parent_emb_t)\n",
    "    child_norms = hit_transformer.manifold.dist0(child_emb_t)\n",
    "    parent_norms = hit_transformer.manifold.dist0(parent_emb_t)\n",
    "    return -(dists + centri_weight * (parent_norms - child_norms))\n",
    "\n",
    "def subsumption_score_ont(ontology_transformer: OntologyTransformer, child_emb: np.ndarray | torch.Tensor, parent_emb: np.ndarray | torch.Tensor, weight_lambda: float = 1.0):\n",
    "    child_emb_t = torch.Tensor(child_emb)\n",
    "    parent_emb_t = torch.Tensor(parent_emb)\n",
    "    return ontology_transformer.score_hierarchy(child_emb_t, parent_emb_t, weight_lambda)\n",
    "\n",
    "\n",
    "def entity_subsumption(u: np.ndarray, vs: np.ndarray, model: HierarchyTransformer, *, weight: float = 0.4):\n",
    "    return np.asarray(subsumption_score_hit(model, u, vs, centri_weight=weight))\n",
    "\n",
    "\n",
    "def concept_subsumption(u: np.ndarray, vs: np.ndarray, model: OntologyTransformer, *, weight: float = 0.4, **kwargs):\n",
    "    return np.asarray(subsumption_score_ont(model, u, vs, weight_lambda=weight))\n",
    "\n",
    "# data mapping: utils\n",
    "\n",
    "def make_signature(obj):\n",
    "  if isinstance(obj, Mapping):\n",
    "    return tuple((key, make_signature(obj[key])) for key in sorted(obj))\n",
    "  elif isinstance(obj, Sequence) and not isinstance(obj, (str, bytes, bytearray)):\n",
    "    return tuple(make_signature(item) for item in obj)\n",
    "  # else:\n",
    "  return obj\n",
    "\n",
    "def unique_unhashable_object_list(obj_xs: list[dict]) -> list[dict]:\n",
    "    object_signatures = set()\n",
    "    unique_obj_list = []\n",
    "    for obj in obj_xs:\n",
    "      signature = make_signature(sorted(obj.items()))\n",
    "      if signature not in object_signatures:\n",
    "        object_signatures.add(signature)\n",
    "        unique_obj_list.append(obj)\n",
    "    return unique_obj_list\n",
    "\n",
    "def obj_max_depth(x: int, obj: Any, key: str = \"depth\") -> int:\n",
    "  return x if x > obj[key] else obj[key]\n",
    "\n",
    "def dcg_exp_relevancy_at_pos(relevancy: int, rank_position: int) -> float:\n",
    "  if relevancy <= 0:\n",
    "    return float(0.0)\n",
    "  numerator = (2**relevancy) - 1\n",
    "  denominator = math.log2(rank_position + 1)\n",
    "  return float(numerator / denominator)\n",
    "\n",
    "def dcg_linear_relevancy_at_pos(relevancy: int, rank_position: int) -> float:\n",
    "  if relevancy <= 0:\n",
    "    return float(0.0)\n",
    "  numerator = relevancy\n",
    "  denominator = math.log2(rank_position + 1)\n",
    "  return float(numerator / denominator)\n",
    "\n",
    "def accumulate(a, b, key='dcg'):\n",
    "  return a + b[key]\n",
    "\n",
    "class Query:\n",
    "  \n",
    "  _query_obj_repr: dict\n",
    "  _query_string: str\n",
    "  _target: dict[str, Union[str, int]]\n",
    "  _entity_mention: dict[str, Union[str, int]]\n",
    "\n",
    "  def __init__(self, query_obj_repr: dict):\n",
    "    self._query_obj_repr = query_obj_repr\n",
    "    self._query_string = query_obj_repr['entity_mention']['entity_literal']\n",
    "    self._target = query_obj_repr['target_entity']\n",
    "    self._entity_mention = query_obj_repr['entity_mention']\n",
    "\n",
    "  def get_query_string(self) -> str:\n",
    "    return self._query_string\n",
    "  \n",
    "  def get_target_iri(self) -> str:\n",
    "    return str(self._target['iri'])\n",
    "  \n",
    "  def get_target_label(self) -> str:\n",
    "    return str(self._target['rdfs:label'])\n",
    "  \n",
    "  def get_target_depth(self) -> int:\n",
    "    return int(self._target['depth'])\n",
    "  \n",
    "  def get_target(self) -> dict:\n",
    "    return self._target\n",
    "\n",
    "\n",
    "# weighted TF-IDF at run-time\n",
    "def aggregate_posting_scores(query_weights, inverted):\n",
    "    scores = {}\n",
    "    for term, weight in query_weights.items():\n",
    "        if term not in inverted:\n",
    "            continue\n",
    "        for doc_id, tfidf_score in inverted[term]:\n",
    "            scores[doc_id] = scores.get(doc_id, 0.0) + weight * tfidf_score\n",
    "    return scores\n",
    "\n",
    "\n",
    "# sort TF-IDF result set\n",
    "def topk(scores: dict[int, float], k: int = 10):\n",
    "    return sorted(scores.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "\n",
    "def build_tf_idf_index(axiom_list: list[str], tfidf_dest: str, *args, **kwargs):\n",
    "    vectoriser = TfidfVectorizer(**kwargs)\n",
    "    doc_term_matrix = vectoriser.fit_transform(axiom_list)\n",
    "    vocab = vectoriser.get_feature_names_out()\n",
    "    # prep for storing to disk: create empty postings struct\n",
    "    inverted_index: dict[str, list[tuple[int, float]]] = {term: [] for term in vocab}\n",
    "    # see: https://matteding.github.io/2019/04/25/sparse-matrices/\n",
    "    coo = coo_matrix(doc_term_matrix)\n",
    "    # populate the inverted index\n",
    "    for row, col, score in zip(coo.row, coo.col, coo.data):\n",
    "        inverted_index[str(vocab[col])].append((int(row), float(score)))\n",
    "    # order: desc\n",
    "    for postings in inverted_index.values():\n",
    "        postings.sort(key=lambda x: x[1], reverse=True)\n",
    "    # save to disk\n",
    "    with open(tfidf_dest, \"wb\") as fp:\n",
    "        pickle.dump(\n",
    "        {\n",
    "            \"vectorizer\": vectoriser,\n",
    "            \"postings\": postings, # type: ignore\n",
    "            \"verbalisations\": axiom_list\n",
    "        },\n",
    "        fp,\n",
    "        protocol=pickle.HIGHEST_PROTOCOL,\n",
    "    )\n",
    "    return vectoriser, inverted_index\n",
    "\n",
    "\n",
    "def build_bm_25_index(concept_list: list[str], bm_25_dest: str = \"bm25-index.pkl\", **kwargs):\n",
    "    tokenised_concepts = parallel_tokenise(concept_list, **kwargs)\n",
    "    bm25 = BM25Okapi(tokenised_concepts)\n",
    "    with open(bm_25_dest, \"wb\") as fp:\n",
    "        pickle.dump({\n",
    "            \"bm25\": bm25,\n",
    "            \"verbalisations\": concept_list,\n",
    "        }, fp, protocol=pickle.HIGHEST_PROTOCOL\n",
    "    )\n",
    "\n",
    "class QueryResult(NamedTuple):\n",
    "  rank: int\n",
    "  iri: str\n",
    "  score: float\n",
    "  verbalisation: str\n",
    "\n",
    "class EquivQuery(Query):\n",
    "  \n",
    "  _equiv_class_expression: Union[str, None]\n",
    "\n",
    "  def __init__(self, query_obj_repr: dict):\n",
    "    super().__init__(query_obj_repr)\n",
    "\n",
    "class SubsumptionQuery(Query):\n",
    "  \n",
    "  _parents: list\n",
    "  _ancestors: list\n",
    "\n",
    "  def __init__(self, query_obj_repr: dict):\n",
    "    super().__init__(query_obj_repr)\n",
    "    self._set_parents()\n",
    "    self._set_ancestors()\n",
    "\n",
    "  def _set_parents(self):\n",
    "    if self._query_obj_repr['parent_entities'] and len(self._query_obj_repr['parent_entities']) > 0:\n",
    "      self._parents = self._query_obj_repr['parent_entities']\n",
    "    else:\n",
    "      self._parents = []\n",
    "\n",
    "  def _set_ancestors(self):\n",
    "    if self._query_obj_repr['ancestors'] and len(self._query_obj_repr['ancestors']) > 0:\n",
    "      self._ancestors = self._query_obj_repr['ancestors']\n",
    "    else:\n",
    "      self._ancestors = []\n",
    "\n",
    "  def get_parents(self) -> list:\n",
    "    return self._parents\n",
    "  \n",
    "  def get_ancestors(self) -> list:\n",
    "    return self._ancestors\n",
    "  \n",
    "  def get_all_subsumptive_targets(self) -> list:\n",
    "    return [self._target, *self._parents, *self._ancestors]\n",
    "  \n",
    "  def get_unique_subsumptive_targets(self) -> list:\n",
    "    return unique_unhashable_object_list(\n",
    "      self.get_all_subsumptive_targets()\n",
    "    )\n",
    "\n",
    "  def get_sorted_subsumptive_targets(self, key=\"depth\", reverse=False, depth_cutoff=3) -> list:\n",
    "    xs = self.get_all_subsumptive_targets()\n",
    "    xs.sort(key=lambda x: x[key], reverse=reverse)\n",
    "    return xs[:depth_cutoff]\n",
    "  \n",
    "  def get_unique_sorted_subsumptive_targets(self, key=\"depth\", reverse=False, depth_cutoff=3) -> list:\n",
    "    return unique_unhashable_object_list(\n",
    "      self.get_sorted_subsumptive_targets(key=key, reverse=reverse, depth_cutoff=depth_cutoff)\n",
    "    )\n",
    "  \n",
    "  def get_targets_with_dcg(self, type=\"exp\", depth_cutoff=3, **kwargs) -> tuple[float, list[dict]]:\n",
    "    # get targets (target, parents, ancestors) ordered in ascending via depth\n",
    "    targets_asc_depth = self.get_unique_sorted_subsumptive_targets(key=\"depth\", depth_cutoff=depth_cutoff)\n",
    "    # increase depth by 1 (offsetting zero-based index)\n",
    "    targets_w_offset = [\n",
    "      {**x, \"depth\": x[\"depth\"] + 1}\n",
    "      for x in targets_asc_depth\n",
    "    ]\n",
    "    # find the max depth (to calculate relevancy): (max_depth - depth_at_pos_k) + zero_based_offset\n",
    "    # which we refer to as: relevancy := ascent height + zero_based_offset\n",
    "    max_target_depth = reduce(\n",
    "      obj_max_depth, \n",
    "      targets_w_offset, \n",
    "      0\n",
    "    )\n",
    "    # calculate relevance for each target (node/parent/ancestor) \n",
    "    targets_with_rel = [\n",
    "      {**x, \"relevance\": (max_target_depth - x[\"depth\"]) + 1}\n",
    "      for x in targets_w_offset\n",
    "    ]\n",
    "    # ensure targets are sorted by relevance\n",
    "    targets_with_rel.sort(key=lambda x: x['relevance'], reverse=True)\n",
    "    # calculate dcg:\n",
    "    if type == \"linear\":\n",
    "      targets_with_dcg = [\n",
    "        {**x, \"dcg\": dcg_linear_relevancy_at_pos(x['relevance'], rank)}\n",
    "        for rank, x in enumerate(targets_with_rel, start=1)\n",
    "      ]\n",
    "    else:\n",
    "      targets_with_dcg = [\n",
    "        {**x, \"dcg\": dcg_exp_relevancy_at_pos(x['relevance'], rank)}\n",
    "        for rank, x in enumerate(targets_with_rel, start=1)\n",
    "      ]\n",
    "    iDCG = reduce(accumulate, targets_with_dcg, 0)\n",
    "    self._idcg = iDCG\n",
    "    self._targets_with_dcg = targets_with_dcg\n",
    "    return iDCG, targets_with_dcg\n",
    "\n",
    "  def get_ideal_dcg(self, type=\"exp\"):\n",
    "    if self._idcg:\n",
    "      return self._idcg\n",
    "    iDCG, targets = self.get_targets_with_dcg()\n",
    "    return iDCG\n",
    "  \n",
    "class QueryObjectMapping:\n",
    "\n",
    "  _loaded: bool\n",
    "  _data_file_path: Path\n",
    "  _data: list\n",
    "  _equiv_queries: list\n",
    "  _subsumpt_queries: list\n",
    "\n",
    "  @validate_call\n",
    "  def __init__(self, json_data_fp: Path):\n",
    "    self._loaded = False\n",
    "    self._data_file_path = json_data_fp\n",
    "    self._load()\n",
    "    self._map()\n",
    "\n",
    "  def _load(self) -> None:\n",
    "    with self._data_file_path.open('r', encoding='utf-8') as fp:\n",
    "      self._data = json.load(fp)\n",
    "    self._loaded = True\n",
    "\n",
    "  @validate_call\n",
    "  def load_from_path(self, json_data_fp: Path) -> None:\n",
    "    # overwrite an existing file path\n",
    "    self._data_file_path = json_data_fp\n",
    "    self._load()\n",
    "\n",
    "  # equivalence_retrieval: bool = True, subsumption_retrieval: bool = False\n",
    "  def _map(self) -> None:\n",
    "    equiv_queries = []\n",
    "    subsumpt_queries = []\n",
    "    for query_obj_repr in self._data:\n",
    "      # if the query obj within the data contains an equiv class\n",
    "      if len(query_obj_repr['equivalent_classes']) > 0:\n",
    "        # we're dealing with an equiv query\n",
    "        equiv_queries.append(EquivQuery(query_obj_repr))\n",
    "      else:\n",
    "        subsumpt_queries.append(SubsumptionQuery(query_obj_repr))\n",
    "    self._equiv_queries = equiv_queries\n",
    "    self._subsumpt_queries = subsumpt_queries\n",
    "\n",
    "  def get_queries(self) -> tuple[list, list]:\n",
    "    return (self._equiv_queries, self._subsumpt_queries)\n",
    "  \n",
    "  def get_subsumpt_queries_with_no_transformations(self):\n",
    "    tmp_subsumpt_queries = copy.deepcopy(self._subsumpt_queries)\n",
    "    result_queries = []\n",
    "    for query in tmp_subsumpt_queries:\n",
    "      if query._entity_mention['transformed_entity_literal_for_type_alignment'] == \"\":\n",
    "        result_queries.append(query)\n",
    "    return result_queries\n",
    "  \n",
    "  def get_subsumpt_queries_with_transformations_only(self):\n",
    "    tmp_subsumpt_queries = copy.deepcopy(self._subsumpt_queries)\n",
    "    result_queries = []\n",
    "    for query in tmp_subsumpt_queries:\n",
    "      if query._entity_mention['transformed_entity_literal_for_type_alignment'] != \"\":\n",
    "        result_queries.append(query)\n",
    "    return result_queries\n",
    "  \n",
    "class BaseRetriever(ABC):\n",
    "    \n",
    "  _verbalisations: list\n",
    "  _meta_map: list\n",
    "\n",
    "  @validate_call\n",
    "  def __init__(self, verbalisations_fp: Path, meta_map_fp: Path):\n",
    "    with open(verbalisations_fp, 'r', encoding='utf-8') as fp:\n",
    "      self._verbalisations = json.load(fp)\n",
    "    with open(meta_map_fp, 'r', encoding='utf-8') as fp:\n",
    "       self._meta_map = json.load(fp)\n",
    "\n",
    "  @abstractmethod\n",
    "  def retrieve(self, query_string: str, *, top_k: int = 10, **kwargs) -> list[QueryResult]:\n",
    "    pass\n",
    "\n",
    "from typing import overload\n",
    "\n",
    "class BaseModelRetriever(BaseRetriever):\n",
    "   \n",
    "  _embeddings: np.ndarray\n",
    "  _candidate_indicies: np.ndarray\n",
    "  _model: Union[SentenceTransformer, HierarchyTransformer, OntologyTransformer]\n",
    "  _score_fn: Callable\n",
    "\n",
    "  @overload\n",
    "  def __init__(self, verbalisations_fp: Path, meta_map_fp: Path, embeddings_fp: Path): ...\n",
    "\n",
    "  @overload\n",
    "  def __init__(self, verbalisations_fp: Path, meta_map_fp: Path, embeddings_fp: Path, *, score_fn: Callable | None = None): ...\n",
    "    \n",
    "  @overload\n",
    "  def __init__(self, verbalisations_fp: Path, meta_map_fp: Path, embeddings_fp: Path, *, score_fn: Callable | None = None, model_fp: Path | None = None): ...\n",
    "    \n",
    "  @overload\n",
    "  def __init__(self, verbalisations_fp: Path, meta_map_fp: Path, embeddings_fp: Path, *, score_fn: Callable | None = None, model_fp: Path | None = None, model_str: str | None = None): ...\n",
    "\n",
    "  @validate_call\n",
    "  def __init__(self, verbalisations_fp: Path, meta_map_fp: Path, embeddings_fp: Path, *, score_fn: Callable | None = None, model_fp: Path | None = None, model_str: str | None = None):\n",
    "    super().__init__(verbalisations_fp, meta_map_fp)\n",
    "    self._embeddings = np.load(embeddings_fp, mmap_mode=\"r\")\n",
    "    self._candidate_indicies = np.arange(len(self._embeddings))\n",
    "    if score_fn:\n",
    "      self.register_score_function(score_fn)\n",
    "    if model_fp:\n",
    "      try:\n",
    "        self.register_local_model(model_fp.expanduser().resolve())\n",
    "      except FileNotFoundError:\n",
    "        self.register_model(str(model_fp))\n",
    "    elif (not model_fp and model_str):\n",
    "      self.register_model(model_str)\n",
    "\n",
    "  def register_score_function(self, score_fn: Callable):\n",
    "    self._score_fn = score_fn\n",
    "\n",
    "  @override\n",
    "  def retrieve(self, query_string: str, *, top_k: int | None = None, reverse_candidate_scores=False, **kwargs) -> list[QueryResult]:\n",
    "    \"\"\"\n",
    "    TODO: 1. add docstring explaining why **kwargs is accepted and pass through to _score_fn\n",
    "          2. add explaination of parameters\n",
    "          3. types (args/return)\n",
    "    \"\"\"\n",
    "    query_embedding = self._embed(query_string)\n",
    "    scored_embeddings = self._score_fn(query_embedding, self._embeddings, **kwargs)\n",
    "    if reverse_candidate_scores and top_k is not None:\n",
    "      top_k_indicies = self._candidate_indicies[np.flip(np.argsort(scored_embeddings))[:top_k]]\n",
    "    elif not reverse_candidate_scores and top_k is not None:\n",
    "      top_k_indicies = self._candidate_indicies[np.argsort(scored_embeddings)[:top_k]]\n",
    "    elif reverse_candidate_scores and top_k is None:\n",
    "      top_k_indicies = self._candidate_indicies[np.flip(np.argsort(scored_embeddings))]\n",
    "    elif not reverse_candidate_scores and top_k is None:\n",
    "      top_k_indicies = self._candidate_indicies[np.argsort(scored_embeddings)]\n",
    "    else:\n",
    "      raise KeyError(\"Valid arguments for reverse_candidate_scores and top_k must be set.\")\n",
    "    results = []\n",
    "    for rank, candidate_index in enumerate(top_k_indicies):\n",
    "      candidate_score = scored_embeddings[candidate_index]\n",
    "      candidate_meta_map = self._meta_map[candidate_index]\n",
    "      candidate_verbalisation = candidate_meta_map['verbalisation']\n",
    "      candidate_iri = candidate_meta_map['iri']\n",
    "      results.append((rank, candidate_iri, candidate_score, candidate_verbalisation))\n",
    "    return results\n",
    "\n",
    "  @abstractmethod\n",
    "  def register_model(self, model: str) -> None:\n",
    "    pass\n",
    "\n",
    "  @abstractmethod\n",
    "  def register_local_model(self, model_fp: Path) -> None:\n",
    "    pass\n",
    "\n",
    "  @abstractmethod\n",
    "  def _embed(self, query_string: str) -> np.ndarray:\n",
    "    pass\n",
    "\n",
    "\n",
    "class HiTRetriever(BaseModelRetriever):\n",
    "\n",
    "  @override\n",
    "  def register_model(self, model: str) -> None:\n",
    "    self._model = HierarchyTransformer.from_pretrained(model)\n",
    "\n",
    "  @override\n",
    "  def register_local_model(self, model_fp: Path) -> None:\n",
    "    self._model = HierarchyTransformer.from_pretrained(str(model_fp.expanduser().resolve()))  \n",
    "\n",
    "  @override\n",
    "  def _embed(self, query_string: str) -> np.ndarray:\n",
    "    return (self._model.encode(\n",
    "      [query_string]\n",
    "    ).astype(\"float32\"))[0] # type: ignore\n",
    "  \n",
    "\n",
    "class OnTRetriever(BaseModelRetriever):\n",
    "\n",
    "  @override\n",
    "  def register_model(self, model: str) -> None:\n",
    "    self._model = OntologyTransformer.load(model)\n",
    "\n",
    "  @override\n",
    "  def register_local_model(self, model_fp: Path) -> None:\n",
    "    self._model = OntologyTransformer.load(str(model_fp.expanduser().resolve()))\n",
    "\n",
    "  @override\n",
    "  def _embed(self, query_string: str) -> np.ndarray:\n",
    "    return (self._model.encode_concept(\n",
    "      [query_string]\n",
    "    ).astype(\"float32\"))[0] # type: ignore\n",
    "\n",
    "\n",
    "class SBERTRetriever(BaseModelRetriever):\n",
    "\n",
    "  @override\n",
    "  def register_model(self, model: str) -> None:\n",
    "    self._model = SentenceTransformer.load(model)\n",
    "\n",
    "  @override\n",
    "  def register_local_model(self, model_fp: Path) -> None:\n",
    "    self._model = SentenceTransformer.load(str(model_fp.expanduser().resolve()))\n",
    "\n",
    "  @override\n",
    "  def _embed(self, query_string: str) -> np.ndarray:\n",
    "    return (self._model.encode(\n",
    "      [query_string]\n",
    "    ).astype(\"float32\"))[0] # type: ignore\n",
    "  \n",
    "\n",
    "def custom_mixed_product_distance(d_ont_32: np.ndarray, d_ont_128: np.ndarray, d_sbert: np.ndarray, \n",
    "                                  sigma: tuple[float, float, float] = (1.0, 1.0, 1.0),\n",
    "                                  to_similarity: bool = True, kernel: str = \"exp\") -> np.ndarray:\n",
    "    \n",
    "    sigma_hit, sigma_ont, sigma_sbert = sigma\n",
    "    d2 = (sigma_hit * d_ont_32)**2 + (sigma_ont * d_ont_128)**2 + (sigma_sbert * d_sbert)**2\n",
    "    if kernel == \"dist\":\n",
    "      return np.sqrt(d2)\n",
    "    if kernel == \"exp\":\n",
    "      return np.exp(-np.sqrt(d2)) # rbf\n",
    "    if kernel in {\"inv\", \"inverse\"}:\n",
    "        return 1.0 / (1.0 + d2) # inverse‑quad\n",
    "    raise ValueError(\"no valid kernel given\")\n",
    "\n",
    "\n",
    "class CustomMixedModelRetriever(BaseRetriever):\n",
    "\n",
    "    _ont_model_32: OntologyTransformer\n",
    "    _ont_model_128: OntologyTransformer\n",
    "    _sbert_model: SentenceTransformer\n",
    "\n",
    "    _ont_embs_32:   np.ndarray\n",
    "    _ont_embs_128:   np.ndarray\n",
    "    _sbert_embs: np.ndarray\n",
    "\n",
    "    _sigma: np.ndarray\n",
    "\n",
    "    def __init__(self, verbalisations_fp: Path, meta_map_fp: Path, *,\n",
    "        ont_model_32: OntologyTransformer, ont_32_embeddings_fp: Path,\n",
    "        ont_model_128: OntologyTransformer, ont_128_embeddings_fp: Path,\n",
    "        sbert_model: SentenceTransformer, sbert_embeddings_fp: Path,\n",
    "        sigma: tuple[float, float, float] = (1.0, 1.0, 1.0),\n",
    "        kernel: str = \"exp\") -> None:\n",
    "\n",
    "        super().__init__(verbalisations_fp, meta_map_fp)\n",
    "\n",
    "        self._ont_model_32 = ont_model_32\n",
    "        self._ont_model_128 = ont_model_128\n",
    "        self._sbert_model = sbert_model\n",
    "\n",
    "        self._ont_embs_32 = np.load(ont_32_embeddings_fp, mmap_mode=\"r\")\n",
    "        self._ont_embs_128 = np.load(ont_128_embeddings_fp, mmap_mode=\"r\")\n",
    "        self._sbert_embs = np.load(sbert_embeddings_fp, mmap_mode=\"r\")\n",
    "\n",
    "        assert len(self._ont_embs_32) == len(self._ont_embs_128) == len(self._sbert_embs), \\\n",
    "            \"all embedding files must contain the same number of rows\"\n",
    "\n",
    "        self._candidate_indices = np.arange(len(self._ont_embs_32))\n",
    "        self._sigma  = np.asarray(sigma, dtype=np.float32)\n",
    "        self._kernel = kernel\n",
    "\n",
    "    def set_sigma(self, sigma: tuple[float, float, float]) -> None:\n",
    "        self._sigma = np.asarray(sigma, dtype=np.float32)\n",
    "\n",
    "    def get_sigma(self) -> tuple[float, float, float]:\n",
    "        return tuple(float(x) for x in self._sigma) # type: ignore\n",
    "\n",
    "    def retrieve(self, query_string: str, *, top_k: int | None = None, reverse_candidate_scores: bool = False, **kwargs) -> list[QueryResult]:\n",
    "\n",
    "        q_ont_32 = self._ont_model_32.encode_concept([query_string])[0]\n",
    "        q_ont_128 = self._ont_model_128.encode_concept([query_string])[0]\n",
    "        q_sbert = self._sbert_model.encode([query_string], normalize_embeddings=True)[0]\n",
    "\n",
    "        d_ont_32 = batch_poincare_dist_with_adaptive_curv_k(q_ont_32, self._ont_embs_32, self._ont_model_32)\n",
    "        d_ont_128 = batch_poincare_dist_with_adaptive_curv_k(q_ont_128, self._ont_embs_128, self._ont_model_128)\n",
    "        d_sbert = batch_euclidian_l2_distance(q_sbert, self._sbert_embs)\n",
    "\n",
    "        scores = custom_mixed_product_distance(\n",
    "            d_ont_32=d_ont_32,\n",
    "            d_ont_128=d_ont_128,\n",
    "            d_sbert=d_sbert,\n",
    "            sigma=tuple(self._sigma),\n",
    "            to_similarity=True,\n",
    "            kernel=self._kernel,\n",
    "        )\n",
    "\n",
    "        if reverse_candidate_scores and top_k is not None:\n",
    "            top_idx = np.argsort(scores)[:top_k]\n",
    "        elif not reverse_candidate_scores and top_k is not None:\n",
    "            top_idx = np.argsort(-scores)[:top_k]\n",
    "        elif reverse_candidate_scores and top_k is None:\n",
    "            top_idx = np.argsort(scores)\n",
    "        elif not reverse_candidate_scores and top_k is None:\n",
    "            top_idx = np.argsort(-scores)\n",
    "        else:\n",
    "            raise KeyError(\"Invalid Argument Exception.\")\n",
    "\n",
    "        results: list[QueryResult] = []\n",
    "        for rank, idx in enumerate(top_idx):\n",
    "            meta = self._meta_map[idx]\n",
    "            results.append(\n",
    "                QueryResult(\n",
    "                    rank = rank,\n",
    "                    iri = meta[\"iri\"],\n",
    "                    score = float(scores[idx]),\n",
    "                    verbalisation = meta[\"verbalisation\"],\n",
    "                )\n",
    "            )\n",
    "        return results\n",
    "    \n",
    "\n",
    "from functools import reduce\n",
    "from typing import Any\n",
    "import math\n",
    "\n",
    "\n",
    "def dcg_exp_relevancy_at_pos(relevancy: int, rank_position: int) -> float:\n",
    "  if relevancy <= 0:\n",
    "    return float(0.0)\n",
    "  numerator = (2**relevancy) - 1\n",
    "  denominator = math.log2(rank_position + 1)\n",
    "  return float(numerator / denominator)\n",
    "\n",
    "\n",
    "def add(a, b, key='dcg'):\n",
    "  return a + b[key]\n",
    "\n",
    "\n",
    "def compute_ndcg_at_k(results: list[tuple[int, str, float, str]], targets_with_dcg_exp: list[dict], k: int = 20) -> float:\n",
    "  relevance_map = {target['iri']: target['relevance'] for target in targets_with_dcg_exp}\n",
    "  dcg = 0.0\n",
    "  for rank, (idx, iri, score, label) in enumerate(results[:k], start=1):\n",
    "    rel = relevance_map.get(iri, 0)\n",
    "    dcg += dcg_exp_relevancy_at_pos(rel, rank)\n",
    "  ideal_dcg = sum(target['dcg'] for target in targets_with_dcg_exp[:k])\n",
    "  if ideal_dcg == 0:\n",
    "    return 0.0\n",
    "  \n",
    "  return dcg / ideal_dcg\n",
    "\n",
    "\n",
    "data_dir = \"./data\"\n",
    "entity_lexicon_fp = Path(f\"{data_dir}/snomed_entity_lexicon_2025.json\")\n",
    "verbalisation_list_fp = Path(f\"{data_dir}/verbalisations_2025.json\")\n",
    "entity_map_fp = Path(f\"{data_dir}/entity_map_2025.json\")\n",
    "entity_mappings_list_fp = Path(f\"{data_dir}/entity_mappings_2025.json\")\n",
    "\n",
    "entity_lexicon = load_json(entity_lexicon_fp)\n",
    "iris = entity_lexicon.keys()\n",
    "entity_map = {}\n",
    "entity_verbalisation_list = []\n",
    "list_of_entity_mappings = []\n",
    "\n",
    "for entity_idx, entity_iri in enumerate(tqdm(iris)):\n",
    "    entity_map[str(entity_idx)] = {\n",
    "        \"mapping_id\": str(entity_idx),\n",
    "        \"label\": entity_lexicon[entity_iri].get('name'), # type: ignore\n",
    "        \"verbalisation\": strip_parens(str(entity_lexicon[entity_iri].get('name'))).lower(), # type: ignore\n",
    "        \"iri\": entity_iri\n",
    "    }\n",
    "    entity_verbalisation_list.append(strip_parens(str(entity_lexicon[entity_iri].get('name'))).lower()) # type: ignore\n",
    "    list_of_entity_mappings.append(entity_map[str(entity_idx)])\n",
    "\n",
    "save_json(verbalisation_list_fp, entity_verbalisation_list)\n",
    "save_json(entity_map_fp, entity_map)\n",
    "save_json(entity_mappings_list_fp, list_of_entity_mappings)\n",
    "\n",
    "\n",
    "sbert_plm_hf_string = \"all-MiniLM-L12-v2\"\n",
    "sbert_plm_encoder = SentenceTransformer.load(sbert_plm_hf_string)\n",
    "\n",
    "ont_anatomy_23_pred_model_fp = \"models/prediction/OnTr-all-MiniLM-L12-v2-ANATOMY\"\n",
    "ont_anatomy_23_pred_encoder = OntologyTransformer.load(ont_anatomy_23_pred_model_fp)\n",
    "\n",
    "ontr_snomed_25_uni_model_fp = './models/OnTr-snomed25-uni'\n",
    "ontr_snomed_25_uni_encoder = OntologyTransformer.load(ontr_snomed_25_uni_model_fp)\n",
    "\n",
    "ontr_snomed_minified_model_fp = './models/OnTr-minified-64'\n",
    "ontr_snomed_encoder = OntologyTransformer.load(ontr_snomed_minified_model_fp)\n",
    "\n",
    "embeddings_dir = \"./embeddings\"\n",
    "save_json(Path(f\"{embeddings_dir}/axiom-verbalisations.json\"), entity_verbalisation_list)\n",
    "save_json(Path(f\"{embeddings_dir}/axiom-mappings.json\"), list_of_entity_mappings)\n",
    "\n",
    "common_map = Path(\"./embeddings/axiom-mappings.json\")\n",
    "common_verbalisations = Path(\"./embeddings/axiom-verbalisations.json\")\n",
    "embeddings_dir = \"./embeddings\"\n",
    "\n",
    "sbert_plm_embs = np.load(f\"{embeddings_dir}/sbert-plm-embeddings.npy\", mmap_mode=\"r\")\n",
    "hit_snomed_23_embs = np.load(f\"{embeddings_dir}/hit-snomed-23-embeddings.npy\", mmap_mode=\"r\")\n",
    "hit_snomed_25_embs = np.load(f\"{embeddings_dir}/hit-snomed-25-embeddings.npy\", mmap_mode=\"r\")\n",
    "ont_anatomy_23_pred_embs = np.load(f\"{embeddings_dir}/ont-anatomy-23-pred-embeddings.npy\", mmap_mode=\"r\")\n",
    "ont_snomed_25_latest_embs = np.load(f\"{embeddings_dir}/ont-snomed-25-latest-embeddings.npy\", mmap_mode=\"r\")\n",
    "ont_minified_embs = np.load(f\"{embeddings_dir}/ont-snomed-minified-embeddings.npy\", mmap_mode=\"r\")\n",
    "\n",
    "## SBERT Model ##\n",
    "\n",
    "sbert_ret_plm_w_cosine_sim = SBERTRetriever(\n",
    "  embeddings_fp=Path(f\"{embeddings_dir}/sbert-plm-embeddings.npy\"),\n",
    "  meta_map_fp=common_map,\n",
    "  verbalisations_fp=common_verbalisations,\n",
    "  model_str=\"all-MiniLM-L12-v2\",\n",
    "  score_fn=batch_cosine_similarity\n",
    ")\n",
    "\n",
    "sbert_ret_plm_w_euclid_dist = SBERTRetriever(\n",
    "  embeddings_fp=Path(f\"{embeddings_dir}/sbert-plm-embeddings.npy\"),\n",
    "  meta_map_fp=common_map,\n",
    "  verbalisations_fp=common_verbalisations,\n",
    "  model_str=\"all-MiniLM-L12-v2\",\n",
    "  score_fn=batch_euclidian_l2_distance\n",
    ")\n",
    "\n",
    "## HiT Models ##\n",
    "\n",
    "hit_ret_snomed_23_w_hyp_dist = HiTRetriever(\n",
    "  embeddings_fp=Path(f\"{embeddings_dir}/hit-snomed-23-embeddings.npy\"),\n",
    "  meta_map_fp=common_map,\n",
    "  verbalisations_fp=common_verbalisations,\n",
    "  model_str=\"Hierarchy-Transformers/HiT-MiniLM-L12-SnomedCT\",\n",
    "  score_fn=batch_poincare_dist_with_adaptive_curv_k\n",
    ")\n",
    "\n",
    "hit_ret_snomed_23_w_ent_sub = hit_ret_snomed_23_w_hyp_dist = HiTRetriever(\n",
    "  embeddings_fp=Path(f\"{embeddings_dir}/hit-snomed-23-embeddings.npy\"),\n",
    "  meta_map_fp=common_map,\n",
    "  verbalisations_fp=common_verbalisations,\n",
    "  model_str=\"Hierarchy-Transformers/HiT-MiniLM-L12-SnomedCT\",\n",
    "  score_fn=entity_subsumption\n",
    ")\n",
    "\n",
    "hit_snomed_23_model = HierarchyTransformer.from_pretrained(\"Hierarchy-Transformers/HiT-MiniLM-L12-SnomedCT\")\n",
    "\n",
    "# HiT-SNOMED-25 #\n",
    "\n",
    "hit_SNOMED25_model_path = Path('./models/HiT-mixed-SNOMED-25/final')\n",
    "\n",
    "hit_ret_snomed_25_w_hyp_dist = HiTRetriever(\n",
    "  embeddings_fp=Path(f\"{embeddings_dir}/hit-snomed-25-embeddings.npy\"),\n",
    "  meta_map_fp=common_map,\n",
    "  verbalisations_fp=common_verbalisations,\n",
    "  model_fp=hit_SNOMED25_model_path,\n",
    "  score_fn=batch_poincare_dist_with_adaptive_curv_k\n",
    ")\n",
    "\n",
    "hit_ret_snomed_25_w_ent_sub = HiTRetriever(\n",
    "  embeddings_fp=Path(f\"{embeddings_dir}/hit-snomed-25-embeddings.npy\"),\n",
    "  meta_map_fp=common_map,\n",
    "  verbalisations_fp=common_verbalisations,\n",
    "  model_fp=hit_SNOMED25_model_path,\n",
    "  score_fn=entity_subsumption\n",
    ")\n",
    "\n",
    "hit_snomed_25_model = HierarchyTransformer.from_pretrained(str(hit_SNOMED25_model_path.expanduser().resolve()))\n",
    "\n",
    "## ONT - ANATONMY PREDICTION\n",
    "\n",
    "ont_anatonmy_pred_model_path = Path(\"models/prediction/OnTr-all-MiniLM-L12-v2-ANATOMY\")\n",
    "\n",
    "ont_ret_anatomy_pred_w_hyp_dist = OnTRetriever(\n",
    "  embeddings_fp=Path(f\"{embeddings_dir}/ont-anatomy-23-pred-embeddings.npy\"),\n",
    "  meta_map_fp=common_map,\n",
    "  verbalisations_fp=common_verbalisations,\n",
    "  model_fp=ont_anatonmy_pred_model_path,\n",
    "  score_fn=batch_poincare_dist_with_adaptive_curv_k\n",
    ")\n",
    "\n",
    "ont_ret_anatomy_pred_w_con_sub = OnTRetriever(\n",
    "  embeddings_fp=Path(f\"{embeddings_dir}/ont-anatomy-23-pred-embeddings.npy\"),\n",
    "  meta_map_fp=common_map,\n",
    "  verbalisations_fp=common_verbalisations,\n",
    "  model_fp=ont_anatonmy_pred_model_path,\n",
    "  score_fn=concept_subsumption\n",
    ")\n",
    "\n",
    "ont_anatomy_model_pred = OntologyTransformer.load(str(ont_anatonmy_pred_model_path.expanduser().resolve()))\n",
    "\n",
    "\n",
    "\n",
    "## OnTr - snomed_25 (Hui Trained)\n",
    "\n",
    "ont_snomed_25_updated_model_path = Path(\"./models/OnTr-snomed25-uni\")\n",
    "\n",
    "ont_ret_snomed_25_updtd_w_hyp_dist = OnTRetriever(\n",
    "    embeddings_fp=Path(f\"{embeddings_dir}/ont-snomed-25-latest-embeddings.npy\"),\n",
    "    meta_map_fp=common_map,\n",
    "    verbalisations_fp=common_verbalisations,\n",
    "    model_fp=ont_snomed_25_updated_model_path,\n",
    "    score_fn=batch_poincare_dist_with_adaptive_curv_k\n",
    ")\n",
    "\n",
    "ont_ret_snomed_25_updtd_w_con_sub = OnTRetriever(\n",
    "    embeddings_fp=Path(f\"{embeddings_dir}/ont-snomed-25-latest-embeddings.npy\"),\n",
    "    meta_map_fp=common_map,\n",
    "    verbalisations_fp=common_verbalisations,\n",
    "    model_fp=ont_snomed_25_updated_model_path,\n",
    "    score_fn=concept_subsumption\n",
    ")\n",
    "\n",
    "ont_snomed_25_updtd_model = OntologyTransformer.load(str(ont_snomed_25_updated_model_path.expanduser().resolve()))\n",
    "\n",
    "## OnTr snomed minified ##\n",
    "\n",
    "ontr_snomed_minified_model_fp = Path('./models/OnTr-minified-64')\n",
    "\n",
    "ontr_ret_snomed_minified_w_hyp_dist = OnTRetriever(\n",
    "    embeddings_fp=Path(f\"{embeddings_dir}/ont-snomed-minified-embeddings.npy\"),\n",
    "    meta_map_fp=common_map,\n",
    "    verbalisations_fp=common_verbalisations,\n",
    "    model_fp=ontr_snomed_minified_model_fp,\n",
    "    score_fn=batch_poincare_dist_with_adaptive_curv_k\n",
    ")\n",
    "\n",
    "ontr_ret_snomed_minified_w_con_sub = OnTRetriever(\n",
    "    embeddings_fp=Path(f\"{embeddings_dir}/ont-snomed-minified-embeddings.npy\"),\n",
    "    meta_map_fp=common_map,\n",
    "    verbalisations_fp=common_verbalisations,\n",
    "    model_fp=ontr_snomed_minified_model_fp,\n",
    "    score_fn=concept_subsumption\n",
    ")\n",
    "\n",
    "ontr_minified_model = OntologyTransformer.load(str(ontr_snomed_minified_model_fp.expanduser().resolve()))\n",
    "\n",
    "## Tripple Mini OnT-Mini + SBERT Product Manifold\n",
    "\n",
    "ont_m_32_emb_fp    = Path(f\"{embeddings_dir}/ont-snomed-minified-32-embeddings.npy\")\n",
    "ont_m_128_emb_fp   = Path(f\"{embeddings_dir}/ont-snomed-minified-128-embeddings.npy\")\n",
    "sbert_emb_fp       = Path(\"./embeddings/sbert-plm-embeddings.npy\")\n",
    "\n",
    "product_ont_model_32  = OntologyTransformer.load('./models/OnTr-m-32')\n",
    "product_ont_model_128 = OntologyTransformer.load('./models/OnTr-m-128')\n",
    "product_sbert_model   = SentenceTransformer(\"all-MiniLM-L12-v2\")\n",
    "\n",
    "mixed_ret_mini = CustomMixedModelRetriever(\n",
    "    verbalisations_fp = common_verbalisations,\n",
    "    meta_map_fp = common_map,\n",
    "    ont_model_32 = product_ont_model_32,\n",
    "    ont_32_embeddings_fp = ont_m_32_emb_fp,\n",
    "    ont_model_128 = product_ont_model_128,\n",
    "    ont_128_embeddings_fp = ont_m_128_emb_fp,\n",
    "    sbert_model = product_sbert_model,\n",
    "    sbert_embeddings_fp = sbert_emb_fp,\n",
    "    sigma = (1.0, 1.0, 0.35),\n",
    "    kernel = \"exp\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88d8ecee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, LogitsProcessorList\n",
    "from pydantic import BaseModel\n",
    "from typing import Any, Callable\n",
    "from pathlib import Path\n",
    "from logits_processor_zoo.transformers import (\n",
    "  CiteFromPromptLogitsProcessor,\n",
    "  MultipleChoiceLogitsProcessor,\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8f0cd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(file_path: Path) -> dict[str, str]:\n",
    "  with file_path.open('r', encoding='utf-8') as fp:\n",
    "    return json.load(fp)\n",
    "\n",
    "def get_dataset(dataset_key: str, benchmark_data: dict):\n",
    "  return benchmark_data[dataset_key]\n",
    "\n",
    "def get_question_obj(question_id: str, dataset: dict):\n",
    "  return dataset[question_id]\n",
    "\n",
    "def get_dataset_question_mapping(dataset_key: str, benchmark_data: dict):\n",
    "  data = get_dataset(dataset_key, benchmark_data)\n",
    "  mapping = {}\n",
    "  for question_index, question_id in enumerate(data):\n",
    "    mapping[question_index] = question_id\n",
    "  return mapping\n",
    "\n",
    "def get_question_str(question_id: str, dataset: dict):\n",
    "  return dataset[question_id]['question']\n",
    "\n",
    "def get_question_opts(question_id: str, dataset: dict):\n",
    "  return dataset[question_id]['options']\n",
    "\n",
    "def get_question_ans(question_id: str, dataset: dict):\n",
    "  return dataset[question_id]['answer']\n",
    "\n",
    "def get_dataset_names(benchmark_data: dict):\n",
    "  return list(benchmark_data.keys())\n",
    "\n",
    "def get_question_count(dataset_name: str, benchmark_data: dict):\n",
    "  return len(benchmark_data[dataset_name])\n",
    "\n",
    "def get_random_question_sample(benchmark_data: dict, allowable_datasets: list[str] = ['medqa', 'medmcqa', 'pubmedqa', 'bioasq', 'mmlu']):\n",
    "  random_dataset_name = allowable_datasets[random.randint(0, len(allowable_datasets) - 1)]\n",
    "  dataset_question_mapping = get_dataset_question_mapping(random_dataset_name, benchmark_data)\n",
    "  # ^ provides a map from custom indicies used to access questions specific to each dataset\n",
    "  dataset_questons_xs = benchmark_data[random_dataset_name]\n",
    "  random_question_index = dataset_question_mapping[random.randint(0, len(dataset_question_mapping) - 1)]\n",
    "  return dataset_questons_xs[random_question_index]\n",
    "\n",
    "def xs_of_all_questions(benchmark_data: dict, allowable_datasets: list[str] = ['medqa', 'medmcqa', 'pubmedqa', 'bioasq', 'mmlu']):\n",
    "  xs = []\n",
    "  for dataset_name in allowable_datasets:\n",
    "    dataset_question_mappings = get_dataset_question_mapping(dataset_name, benchmark_data)\n",
    "    question_list = benchmark_data[dataset_name]\n",
    "    for itr, mapping_idx in dataset_question_mappings.items():\n",
    "      xs.append(question_list[mapping_idx])\n",
    "  return xs\n",
    "\n",
    "def get_question_entity_mentions(entity_mention_data: dict, dataset: str, question_id: str):\n",
    "  for question in entity_mention_data[\"questions\"]:\n",
    "    if question['source_dataset'] == dataset and question['question_id'] == question_id:\n",
    "      return question['entities'] # warning: it is possible to be []\n",
    "\n",
    "def merge_entity_mentions(benchmark_data: dict, biomed_entities: dict, head_entities: dict, allowable_datasets: list[str] = ['medqa', 'medmcqa', 'pubmedqa', 'bioasq', 'mmlu']):\n",
    "  xs = []\n",
    "  for dataset_name in allowable_datasets:\n",
    "    print(f\"Processing {dataset_name} ... \")\n",
    "    dataset_question_mappings = get_dataset_question_mapping(dataset_name, benchmark_data)\n",
    "    question_list = benchmark_data[dataset_name]\n",
    "    for itr, mapping_idx in tqdm(dataset_question_mappings.items()):\n",
    "      question_entities_biomedical = get_question_entity_mentions(biomed_entities, dataset_name, mapping_idx)\n",
    "      question_entities_head = get_question_entity_mentions(head_entities, dataset_name, mapping_idx)\n",
    "      question_list[mapping_idx]['entities'] = []\n",
    "      question_list[mapping_idx]['entities'].extend(question_entities_biomedical)\n",
    "      question_list[mapping_idx]['entities'].extend(question_entities_head)\n",
    "      xs.append(question_list[mapping_idx])\n",
    "  return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d73e7dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mirage_benchmark = load_json(Path(\"./data/MIRAGE/benchmark.json\"))\n",
    "\n",
    "def get_random_mirage_question():\n",
    "  return get_random_question_sample(mirage_benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9e33ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing medqa with 1273 questions ...\n",
      "Processing medmcqa with 4183 questions ...\n",
      "Processing pubmedqa with 500 questions ...\n",
      "Processing bioasq with 618 questions ...\n",
      "Processing mmlu with 1089 questions ...\n",
      "\n",
      "Finished! The maximum number of options in any question is 4\n",
      "The set of option keys consists of the following elements: {'A', 'D', 'C', 'B'}\n"
     ]
    }
   ],
   "source": [
    "mirage_benchmark = load_json(Path(\"./data/MIRAGE/benchmark.json\"))\n",
    "\n",
    "max_options = 0\n",
    "option_keys = set()\n",
    "\n",
    "for dataset in mirage_benchmark:\n",
    "  print(f\"Processing {dataset} with {len(mirage_benchmark[dataset])} questions ...\")\n",
    "  for dataset_question in mirage_benchmark[dataset]:\n",
    "    num_options_in_question = len(mirage_benchmark[dataset][dataset_question]['options'])\n",
    "    options = list(mirage_benchmark[dataset][dataset_question]['options'].keys())\n",
    "    for option in options:\n",
    "      option_keys.add(option)\n",
    "    if num_options_in_question > max_options:\n",
    "      if VERBOSE:\n",
    "        print(f\"New maximum number of options founds: {num_options_in_question}\")\n",
    "      max_options = num_options_in_question\n",
    "\n",
    "print(f\"\\nFinished! The maximum number of options in any question is {max_options}\")\n",
    "print(f\"The set of option keys consists of the following elements: {option_keys}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31b16fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "medqa contains 1273 questions.\n",
      "medmcqa contains 4183 questions.\n",
      "pubmedqa contains 500 questions.\n",
      "bioasq contains 618 questions.\n",
      "mmlu contains 1089 questions.\n",
      "\n",
      "All datasets contain a total of 7663 questions.\n"
     ]
    }
   ],
   "source": [
    "dataset_names = get_dataset_names(mirage_benchmark)\n",
    "\n",
    "medqa_mapping = get_dataset_question_mapping(\"medqa\", mirage_benchmark)\n",
    "medmcqa_mapping = get_dataset_question_mapping(\"medmcqa\", mirage_benchmark)\n",
    "pubmedqa_mapping = get_dataset_question_mapping(\"pubmedqa\", mirage_benchmark)\n",
    "bioasq_mapping = get_dataset_question_mapping(\"bioasq\", mirage_benchmark)\n",
    "mmlu_mapping = get_dataset_question_mapping(\"mmlu\", mirage_benchmark)\n",
    "\n",
    "total_questions = 0\n",
    "for name in dataset_names:\n",
    "  print(f\"{name} contains {get_question_count(name, mirage_benchmark)} questions.\")\n",
    "  total_questions += get_question_count(name, mirage_benchmark)\n",
    "print(f\"\\nAll datasets contain a total of {total_questions} questions.\")\n",
    "\n",
    "if VERBOSE:\n",
    "  random_index = random.randint(0, len(medqa_mapping) - 1)\n",
    "  sample_random_medqa_mapping = medqa_mapping[random_index]\n",
    "  print(random_index, \" -> \", sample_random_medqa_mapping)\n",
    "  print(dataset_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40415a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mirage_questions = xs_of_all_questions(mirage_benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d8995644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def foldr_xs_to_csv(xs: list[str]) -> str:\n",
    "  \"\"\"recursive foldr (right fold) for creating a single csv row\"\"\"\n",
    "  if len(xs) == 1:\n",
    "    return xs[0]\n",
    "  return str(f\"{xs[0]},{foldr_xs_to_csv(xs[1:])}\")\n",
    "\n",
    "\n",
    "def format_options(options: dict[str, str]) -> str:\n",
    "  \"\"\"produces a list of options for inclusion within prompts\"\"\"\n",
    "  return \"\\n\".join(f\"{k}. {opt}\" for k, opt in options.items())\n",
    "\n",
    "\n",
    "def opt_letters(options: dict[str, str]) -> str:\n",
    "  \"\"\"produces a comma seperated list of a dicts keys, e.g. A,B,C,D\"\"\"\n",
    "  return foldr_xs_to_csv(list(options.keys()))\n",
    "\n",
    "\n",
    "def prompt_template_no_rag(question: str, options: dict[str, str], **kwargs) -> str:\n",
    "  \"\"\"produce a simple biomedical question answering (MC) template, discards additional kwargs (e.g. answer)\"\"\"\n",
    "  return (\n",
    "    \"You are a helpful medical expert, and your task is to answer a multi-choice medical question. Your response will be used for research purposes only.\\n\"\n",
    "    f\"Return only the letter of the best answer ({opt_letters(options)}).\\n\\n\"\n",
    "    f\"Here is the question:\\n{question}\\n\\n\"\n",
    "    f\"Here are the potential choices:\\n{format_options(options)}\\n\\n\"\n",
    "    \"Answer (letter only): \"\n",
    "  )\n",
    "\n",
    "\n",
    "def prompt_template_with_axioms(question: str, options: dict[str, str], axioms: list[str], **kwargs) -> str:\n",
    "  \"\"\"produce a biomedical MCQA prompt for RAG, discards additional kwargs (e.g. answer)\"\"\"\n",
    "  axiomatic_context = \"\\n\".join(axiom for axiom in axioms)\n",
    "  return (\n",
    "    \"You are a helpful medical expert, your task is to answer a multi-choice medical question.\\n\"\n",
    "    f\"Return only the letter of the best answer ({opt_letters(options)}).\\n\\n\"\n",
    "    f\"Helpful context:\\n{axiomatic_context}\\n\\n\"\n",
    "    f\"Here is the question:\\n{question}\\n\\n\"\n",
    "    f\"Here are the potential choices:\\n{format_options(options)}\\n\\n\"\n",
    "    \"Answer (letter only): \"\n",
    "  )\n",
    "\n",
    "\n",
    "# TODO: create a prompt template registry (singleton container)\n",
    "PROMPT_TEMPLATES = {\n",
    "  \"mirage_mcqa_no_rag\": prompt_template_no_rag,\n",
    "  \"mirage_mcqa_axiom_rag\": prompt_template_with_axioms\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79b183e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLMs\n",
    "\n",
    "class MistralLLM:\n",
    "\n",
    "  _hf_id: str\n",
    "  _model: Any\n",
    "  _tokenizer: Any\n",
    "  _callable_prompt_templates: dict[str, Callable]\n",
    "\n",
    "  def __init__(self, hf_identifier: str, **kwargs):\n",
    "    self._hf_id = hf_identifier\n",
    "    self._model = None\n",
    "    self._tokenizer = None\n",
    "    self._callable_prompt_templates = {}\n",
    "    \n",
    "  def load_model(self, **kwargs):\n",
    "    self._model = AutoModelForCausalLM.from_pretrained(self._hf_id, **kwargs)\n",
    "    self._model.eval()\n",
    "    return self\n",
    "    \n",
    "  def load_tokenizer(self, **kwargs):\n",
    "    self._tokenizer = AutoTokenizer.from_pretrained(self._hf_id, **kwargs)\n",
    "    if self._tokenizer.pad_token_id is None:\n",
    "      self._tokenizer.pad_token = self._tokenizer.eos_token\n",
    "    return self\n",
    "\n",
    "  def register_generation_config(self, **kwargs):\n",
    "    self._model.generation_config = GenerationConfig(**kwargs)\n",
    "    return self\n",
    "\n",
    "  def register_prompt_template_fn(self, callback_key: str, fn: Callable):\n",
    "    self._callable_prompt_templates[callback_key] = fn\n",
    "    return self\n",
    "\n",
    "  @torch.inference_mode()\n",
    "  def generate(self, prompt: str, **kwargs):\n",
    "    inputs = self._tokenizer(\n",
    "      prompt,\n",
    "      return_tensors=\"pt\", \n",
    "    ).to(self._model.device)\n",
    "    # generate output\n",
    "    out = self._model.generate(\n",
    "      **inputs,\n",
    "      **kwargs\n",
    "    ) # decode & return\n",
    "    return self._tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "  def generate_inject_template(self, template_key: str, template_args: dict, **kwargs):\n",
    "    template_fn = self._callable_prompt_templates[template_key]\n",
    "    prompt = template_fn(**template_args)\n",
    "    return self.generate(prompt, **kwargs)\n",
    "  \n",
    "  def generate_constrain_logits(self, prompt: str, logits_processor_list: list | None = None, max_tokens: int = 1000, **kwargs):\n",
    "    if logits_processor_list is None:\n",
    "      logits_processor_list = []\n",
    "    return self.generate(\n",
    "      prompt,\n",
    "      max_new_tokens = max_tokens,\n",
    "      min_new_tokens = 1,\n",
    "      logits_processor = LogitsProcessorList(logits_processor_list),\n",
    "      **kwargs\n",
    "    )\n",
    "  \n",
    "  # TODO: clean this up a little bit, fn should accept the delimiter\n",
    "  def generate_inject_template_and_constrain_logits_for_mcqa(self, template_key: str, template_args: dict, **kwargs):\n",
    "    mclp = MultipleChoiceLogitsProcessor(\n",
    "      tokenizer=self._tokenizer,\n",
    "      choices=list(template_args[\"options\"].keys()),\n",
    "      delimiter=\".\"\n",
    "    )\n",
    "    template_fn = self._callable_prompt_templates[template_key]\n",
    "    prompt = template_fn(**template_args)\n",
    "    return self.generate_constrain_logits(prompt, [mclp], max_tokens=1, **kwargs)\n",
    "\n",
    "  # TODO: fix this, simply grabbing the last character from the string *may* result in inaccuracies (works fine for now!)\n",
    "  def generate_single_letter_for_mcqa(self, template_key: str, template_args: dict, **kwargs):\n",
    "    response = self.generate_inject_template_and_constrain_logits_for_mcqa(template_key, template_args, **kwargs)\n",
    "    return str(response)[len(str(response)) - 1:]\n",
    "\n",
    "\n",
    "\n",
    "class BaseEntitySelector:\n",
    "\n",
    "  _STOPWORDS = [\n",
    "    \"patient\",\"pt\",\"pts\",\"person\",\"people\",\"individual\",\"individuals\",\n",
    "    \"male\",\"female\",\"man\",\"woman\",\"boy\",\"girl\",\"child\",\"children\",\"kid\",\n",
    "    \"infant\",\"newborn\",\"neonate\",\"adult\",\"elderly\",\"senior\",\"parent\",\"parents\",\n",
    "    \"he\",\"she\",\"they\",\"them\",\"him\",\"her\",\"his\",\"hers\",\"their\",\"theirs\",\n",
    "    \"you\",\"your\",\"yours\",\"we\",\"our\",\"ours\",\"i\",\"me\",\"my\",\"mine\",\"one\",\n",
    "    \"someone\",\"anyone\",\"everyone\",\"nobody\",\"somebody\",\"this\",\"that\",\"these\",\"those\",\n",
    "    \"presented\",\"presents\",\"presenting\",\"complains\",\"complained\",\"reports\",\"reported\",\n",
    "    \"history\",\"h/o\",\"c/o\",\"since\",\"for\",\"with\",\"had\",\"found\",\"noted\",\"developed\",\n",
    "    \"exhibits\",\"demonstrates\",\"shows\",\"reveals\",\"diagnosed\",\"diagnosis\",\"examination\",\"exam\",\n",
    "    \"on\",\"during\",\"before\",\"after\",\"prior\",\n",
    "    \"which\",\"what\",\"when\",\"where\",\"why\",\"how\",\"whose\",\"whom\",\n",
    "    \"true\",\"false\",\"correct\",\"incorrect\",\"appropriate\",\"best\",\"most\",\"least\",\n",
    "    \"except\",\"not\",\"all\",\"following\",\"choose\",\"select\",\"mark\",\"option\",\"options\",\n",
    "    \"both\",\"none\",\"either\",\"neither\",\"above\",\"below\",\n",
    "    \"feature\",\"features\",\"sign\",\"signs\",\"symptom\",\"symptoms\",\"finding\",\"findings\",\n",
    "    \"test\",\"tests\",\"result\",\"results\",\"value\",\"values\",\"level\",\"levels\",\"rate\",\"ratio\",\n",
    "    \"management\",\"treatment\",\"therapy\",\"mechanism\",\"complication\",\"evaluation\",\"investigation\",\"investigations\",\n",
    "    \"method\",\"methods\",\"technique\",\"techniques\",\"approach\",\"approaches\",\"procedure\",\"procedures\",\n",
    "    \"cause\",\"causes\",\"type\",\"types\",\"class\",\"classes\",\"category\",\"categories\",\"group\",\"groups\",\n",
    "    \"age\",\"aged\",\"old\",\"year\",\"years\",\"yr\",\"yrs\",\"month\",\"months\",\"mo\",\"mos\",\n",
    "    \"week\",\"weeks\",\"day\",\"days\",\"hour\",\"hours\",\"hr\",\"hrs\",\"minute\",\"minutes\",\"min\",\"mins\",\n",
    "    \"always\",\"never\",\"usually\",\"commonly\",\"rarely\",\"frequently\",\"sometimes\",\"generally\",\"typically\",\n",
    "    \"mainly\",\"mostly\",\"predominantly\",\"severe\",\"mild\",\"moderate\",\"acute\",\"chronic\",\"subacute\",\"persistent\",\"recurrent\",\n",
    "    \"according\",\"guidelines\",\"classification\",\"defined\",\"definition\",\"called\",\"known\",\"named\",\"term\",\"terminology\",\n",
    "    \"left\",\"right\",\"bilateral\",\"unilateral\",\"anterior\",\"posterior\",\"medial\",\"lateral\",\"superior\",\"inferior\",\n",
    "    \"proximal\",\"distal\",\"upper\",\"lower\",\"central\",\"peripheral\",\n",
    "    \"hospital\",\"clinic\",\"ward\",\"opd\",\"er\",\"icu\",\"casualty\",\n",
    "    \"region\",\"area\",\"part\",\"portion\",\"site\",\"surface\",\"margin\",\"border\",\"apex\",\"base\",\n",
    "    \"volume\",\"pressure\",\"temperature\",\"saturation\",\"score\",\"grade\",\"stage\",\"index\",\n",
    "  ]\n",
    "  _all_mention_results: list[QueryResult]\n",
    "  _retriever: BaseRetriever\n",
    "\n",
    "  def __init__(self, retriever: BaseRetriever):\n",
    "    self._retriever = retriever\n",
    "\n",
    "  def encode_and_rank_candidates(self, entities):\n",
    "    pass\n",
    "\n",
    "  def get_top_candidates(self, top_k=3):\n",
    "    return self._all_mention_results[:top_k]\n",
    "\n",
    "\n",
    "class SubsumptionEntitySelector(BaseEntitySelector):\n",
    "  @override\n",
    "  def encode_and_rank_candidates(self, entities):\n",
    "    self._all_mention_results = []\n",
    "    for mention in entities:\n",
    "      if mention['entity_literal'] in self._STOPWORDS:\n",
    "        continue\n",
    "      # else:\n",
    "      self._all_mention_results.append(\n",
    "        self._retriever.retrieve(mention['entity_literal'], top_k=1, reverse_candidate_scores=True, weight=0.4, model=self._retriever._model)[0]\n",
    "      )\n",
    "    self._all_mention_results.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "\n",
    "class ApproximateNearestNeighbourEntitySelector(BaseEntitySelector):\n",
    "  @override\n",
    "  def encode_and_rank_candidates(self, entities):\n",
    "    self._all_mention_results = []\n",
    "    for mention in entities:\n",
    "      if mention['entity_literal'] in self._STOPWORDS:\n",
    "        continue\n",
    "      # else:\n",
    "      self._all_mention_results.append(\n",
    "        self._retriever.retrieve(mention['entity_literal'], top_k=1, reverse_candidate_scores=False, model=self._retriever._model)[0]\n",
    "      )\n",
    "    self._all_mention_results.sort(key=lambda x: x[2], reverse=False)\n",
    "\n",
    "\n",
    "class SimilarityEntitySelector(BaseEntitySelector):\n",
    "  @override\n",
    "  def encode_and_rank_candidates(self, entities):\n",
    "    self._all_mention_results = []\n",
    "    for mention in entities:\n",
    "      if mention['entity_literal'] in self._STOPWORDS:\n",
    "        continue\n",
    "      # else:\n",
    "      self._all_mention_results.append(\n",
    "        self._retriever.retrieve(mention['entity_literal'], top_k=1, reverse_candidate_scores=True)[0]\n",
    "      )\n",
    "    self._all_mention_results.sort(key=lambda x: x[2], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bca18151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing medqa ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1273/1273 [00:00<00:00, 16618.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing medmcqa ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4183/4183 [00:04<00:00, 1018.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pubmedqa ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 724.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing bioasq ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 618/618 [00:00<00:00, 702.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing mmlu ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1089/1089 [00:01<00:00, 650.71it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "912c2467d8714ae69b7b657eb44afb28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded!\n"
     ]
    }
   ],
   "source": [
    "mirage_benchmark = load_json(Path(\"./data/MIRAGE/benchmark.json\"))\n",
    "mirage_questions = xs_of_all_questions(mirage_benchmark)\n",
    "biomedical_entity_mentions = load_json(Path(\"./data/MIRAGE/benchmark-questions-entities-BIOMED-bionlp13cg.json\"))\n",
    "head_entity_mentions = load_json(Path(\"./data/MIRAGE/benchmark-questions-entities-HEAD.json\"))\n",
    "mirage_questions_with_entity_mentions = merge_entity_mentions(mirage_benchmark, biomedical_entity_mentions, head_entity_mentions)\n",
    "\n",
    "mistral_lm = MistralLLM(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "\n",
    "mistral_lm.load_tokenizer(\n",
    "  use_fast=True\n",
    ").load_model(\n",
    "  device_map=\"auto\",\n",
    "  torch_dtype=torch.bfloat16,\n",
    "  low_cpu_mem_usage=True\n",
    ").register_generation_config(\n",
    "  do_sample=False,\n",
    "  num_beams=1,\n",
    "  pad_token_id=mistral_lm._tokenizer.pad_token_id,\n",
    "  eos_token_id=mistral_lm._tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "mistral_lm.register_prompt_template_fn(\"mirage_mcqa_no_rag\", prompt_template_no_rag)\n",
    "mistral_lm.register_prompt_template_fn(\"mirage_mcqa_axiom_rag\", prompt_template_with_axioms)\n",
    "\n",
    "print(\"Loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c32cb73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7947f076",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_question_sample = get_random_mirage_question()\n",
    "\n",
    "snomed_concept_information: dict = load_json(Path(\"./data/snomed_axioms.json\"))\n",
    "entity_selector = ApproximateNearestNeighbourEntitySelector(ont_ret_snomed_25_updtd_w_hyp_dist)\n",
    "\n",
    "entity_selector.encode_and_rank_candidates(random_question_sample['entities'])\n",
    "entities_for_rag = entity_selector.get_top_candidates(top_k=5)\n",
    "entity_mention_iris_for_rag = []\n",
    "for mention in entities_for_rag:\n",
    "  entity_mention_iris_for_rag.append(mention[1])\n",
    "\n",
    "# obtain axiom verbalisations (or produce `concept cards`) for each IRI for prompt enrichment\n",
    "\n",
    "axiom_verbalisations = []\n",
    "\n",
    "for iri in entity_mention_iris_for_rag:\n",
    "  label: str = snomed_concept_information[iri]['label']\n",
    "  subclass_axioms: list[str] = snomed_concept_information[iri]['verbalization']['subclass_of']\n",
    "  equiv_axioms: list[str] = snomed_concept_information[iri]['verbalization']['equivalent_to']\n",
    "  for idx, axiom in enumerate(subclass_axioms):\n",
    "    axiom_verbalisations.append(f\"{label}: {subclass_axioms[idx]}\")\n",
    "  for idx, axiom in enumerate(equiv_axioms):\n",
    "    axiom_verbalisations.append(f\"{label}: {equiv_axioms[idx]}\")\n",
    "\n",
    "# bind the axioms verbalisations to the question object for prompt injection\n",
    "\n",
    "random_question_sample['axioms'] = axiom_verbalisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bc222c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A previously healthy 29-year-old man comes to the emergency department because of burning with urination for several days. He has also had pain in the right ankle for 3 days and pain and swelling in the left knee for 1 day. Two weeks ago, he had several days of fever and bloody diarrhea, for which he was treated with antibiotics. Examination shows a small left knee effusion and bilateral conjunctival injection. Which of the following is the most likely additional finding in this patient?'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_question_sample['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7bc5ae75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language model response: \n",
      "\n",
      "You are a helpful medical expert, and your task is to answer a multi-choice medical question. Your response will be used for research purposes only.\n",
      "Return only the letter of the best answer (A,B,C,D).\n",
      "\n",
      "Here is the question:\n",
      "A previously healthy 29-year-old man comes to the emergency department because of burning with urination for several days. He has also had pain in the right ankle for 3 days and pain and swelling in the left knee for 1 day. Two weeks ago, he had several days of fever and bloody diarrhea, for which he was treated with antibiotics. Examination shows a small left knee effusion and bilateral conjunctival injection. Which of the following is the most likely additional finding in this patient?\n",
      "\n",
      "Here are the potential choices:\n",
      "A. Circular erythematous rash with central clearing\n",
      "B. Pain on passive extension of the fingers\n",
      "C. Palpable mass in the right lower quadrant\n",
      "D. Tenderness at the insertion of the Achilles tendon\n",
      "\n",
      "Answer (letter only):  A\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "The correct answer is: D\n"
     ]
    }
   ],
   "source": [
    "response = mistral_lm.generate_inject_template_and_constrain_logits_for_mcqa(\"mirage_mcqa_no_rag\", random_question_sample)\n",
    "print(f\"Language model response: \\n\\n{response}\\n\")\n",
    "print(\"-\" * 72)\n",
    "print(f\"The correct answer is: {random_question_sample['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "68dc6188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language model response: \n",
      "\n",
      "You are a helpful medical expert, your task is to answer a multi-choice medical question.\n",
      "Return only the letter of the best answer (A,B,C,D).\n",
      "\n",
      "Helpful context:\n",
      "Examination: is a type of Additional values\n",
      "Conjunctival concretion: defined as Disease with morphology: Focal calcification, site: Conjunctiva.\n",
      "HEA: is a type of Anatomical site notations for tumor staging\n",
      "Age more than 50 years, male: is a type of Age more than 50 years\n",
      "Each: is a type of Additional dosage instructions\n",
      "\n",
      "Here is the question:\n",
      "A previously healthy 29-year-old man comes to the emergency department because of burning with urination for several days. He has also had pain in the right ankle for 3 days and pain and swelling in the left knee for 1 day. Two weeks ago, he had several days of fever and bloody diarrhea, for which he was treated with antibiotics. Examination shows a small left knee effusion and bilateral conjunctival injection. Which of the following is the most likely additional finding in this patient?\n",
      "\n",
      "Here are the potential choices:\n",
      "A. Circular erythematous rash with central clearing\n",
      "B. Pain on passive extension of the fingers\n",
      "C. Palpable mass in the right lower quadrant\n",
      "D. Tenderness at the insertion of the Achilles tendon\n",
      "\n",
      "Answer (letter only):  A\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "The correct answer is: D\n"
     ]
    }
   ],
   "source": [
    "response = mistral_lm.generate_inject_template_and_constrain_logits_for_mcqa(\"mirage_mcqa_axiom_rag\", random_question_sample)\n",
    "print(f\"Language model response: \\n\\n{response}\\n\")\n",
    "print(\"-\" * 72)\n",
    "print(f\"The correct answer is: {random_question_sample['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b867ab",
   "metadata": {},
   "source": [
    "### Example(s) of Single Axiom Verbalisation Injection (k=1) Response:\n",
    "\n",
    "*The correct answer is: B*\n",
    "\n",
    "**Original prompt (\\w response):**\n",
    "\n",
    "    You are a helpful medical expert, and your task is to answer a multi-choice medical question.\n",
    "    Return only the letter of the best answer (A,B,C,D).\n",
    "\n",
    "    Here is the question:\n",
    "    A young girl hospitalised with anorexia nervosa is on treatment, Even after taking adequate food according to the recommended diet plan for last 1 week, there is no gain in weight, what is the next step in management:\n",
    "\n",
    "    Here are the potential choices:\n",
    "    A. Increase fluid intake\n",
    "    B. Observe patient for 2 hours after meal\n",
    "    C. Increase the do se of anxiolytics\n",
    "    D. Increase the caloric intake from 1500 kcal to 2000 kcal per day\n",
    "\n",
    "    Answer (letter only):  D\n",
    "\n",
    "**RAG-based prompt (\\w response):**\n",
    "\n",
    "    You are a helpful medical expert, your task is to answer a multi-choice medical question.\n",
    "    Return only the letter of the best answer (A,B,C,D).\n",
    "\n",
    "    Helpful context:\n",
    "    Atypical anorexia nervosa is a type of Eating disorder\n",
    "\n",
    "    Here is the question:\n",
    "    A young girl hospitalised with anorexia nervosa is on treatment, Even after taking adequate food according to the recommended diet plan for last 1 week, there is no gain in weight, what is the next step in management:\n",
    "\n",
    "    Here are the potential choices:\n",
    "    A. Increase fluid intake\n",
    "    B. Observe patient for 2 hours after meal\n",
    "    C. Increase the do se of anxiolytics\n",
    "    D. Increase the caloric intake from 1500 kcal to 2000 kcal per day\n",
    "\n",
    "    Answer (letter only):  D\n",
    "\n",
    "\n",
    "\n",
    "### Example(s) of Multi-Axiom Verbalisation Injection (k=5) Response:\n",
    "\n",
    "*The correct answer is D*\n",
    "\n",
    "**Original Prompt:**\n",
    "\n",
    "    You are a helpful medical expert, and your task is to answer a multi-choice medical question. Your response will be used for research purposes only.\n",
    "    Return only the letter of the best answer (A,B,C,D).\n",
    "\n",
    "    Here is the question:\n",
    "    A previously healthy 29-year-old man comes to the emergency department because of burning with urination for several days. He has also had pain in the right ankle for 3 days and pain and swelling in the left knee for 1 day. Two weeks ago, he had several days of fever and bloody diarrhea, for which he was treated with antibiotics. Examination shows a small left knee effusion and bilateral conjunctival injection. Which of the following is the most likely additional finding in this patient?\n",
    "\n",
    "    Here are the potential choices:\n",
    "    A. Circular erythematous rash with central clearing\n",
    "    B. Pain on passive extension of the fingers\n",
    "    C. Palpable mass in the right lower quadrant\n",
    "    D. Tenderness at the insertion of the Achilles tendon\n",
    "\n",
    "    Answer (letter only):  A\n",
    "\n",
    "**RAG-based Prompt:**\n",
    "\n",
    "    You are a helpful medical expert, your task is to answer a multi-choice medical question.\n",
    "    Return only the letter of the best answer (A,B,C,D).\n",
    "\n",
    "    Helpful context:\n",
    "    Examination is a type of Additional values\n",
    "    Conjunctival concretion defined as Disease with morphology Focal calcification, site Conjunctiva\n",
    "    HEA is a type of Anatomical site notations for tumor staging\n",
    "    Age more than 50 years, male is a type of Age more than 50 years\n",
    "    Each is a type of Additional dosage instructions\n",
    "\n",
    "    Here is the question:\n",
    "    A previously healthy 29-year-old man comes to the emergency department because of burning with urination for several days. He has also had pain in the right ankle for 3 days and pain and swelling in the left knee for 1 day. Two weeks ago, he had several days of fever and bloody diarrhea, for which he was treated with antibiotics. Examination shows a small left knee effusion and bilateral conjunctival injection. Which of the following is the most likely additional finding in this patient?\n",
    "\n",
    "    Here are the potential choices:\n",
    "    A. Circular erythematous rash with central clearing\n",
    "    B. Pain on passive extension of the fingers\n",
    "    C. Palpable mass in the right lower quadrant\n",
    "    D. Tenderness at the insertion of the Achilles tendon\n",
    "\n",
    "    Answer (letter only):  A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05860418",
   "metadata": {},
   "source": [
    "### Example(s) of Single Axiom Verbalisation Injection (k=1) Response:\n",
    "\n",
    "*The correct answer is: B*\n",
    "\n",
    "**Original prompt (\\w response):**\n",
    "\n",
    "    You are a helpful medical expert, and your task is to answer a multi-choice medical question.\n",
    "    Return only the letter of the best answer (A,B,C,D).\n",
    "\n",
    "    Here is the question:\n",
    "    A young girl hospitalised with anorexia nervosa is on treatment, Even after taking adequate food according to the recommended diet plan for last 1 week, there is no gain in weight, what is the next step in management:\n",
    "\n",
    "    Here are the potential choices:\n",
    "    A. Increase fluid intake\n",
    "    B. Observe patient for 2 hours after meal\n",
    "    C. Increase the do se of anxiolytics\n",
    "    D. Increase the caloric intake from 1500 kcal to 2000 kcal per day\n",
    "\n",
    "    Answer (letter only):  D\n",
    "\n",
    "**RAG-based prompt (\\w response):**\n",
    "\n",
    "    You are a helpful medical expert, your task is to answer a multi-choice medical question.\n",
    "    Return only the letter of the best answer (A,B,C,D).\n",
    "\n",
    "    Helpful context:\n",
    "    Atypical anorexia nervosa is a type of Eating disorder\n",
    "\n",
    "    Here is the question:\n",
    "    A young girl hospitalised with anorexia nervosa is on treatment, Even after taking adequate food according to the recommended diet plan for last 1 week, there is no gain in weight, what is the next step in management:\n",
    "\n",
    "    Here are the potential choices:\n",
    "    A. Increase fluid intake\n",
    "    B. Observe patient for 2 hours after meal\n",
    "    C. Increase the do se of anxiolytics\n",
    "    D. Increase the caloric intake from 1500 kcal to 2000 kcal per day\n",
    "\n",
    "    Answer (letter only):  D\n",
    "\n",
    "\n",
    "\n",
    "### Example(s) of Multi-Axiom Verbalisation Injection (k=5) Response:\n",
    "\n",
    "*The correct answer is D*\n",
    "\n",
    "**Original Prompt:**\n",
    "\n",
    "    You are a helpful medical expert, and your task is to answer a multi-choice medical question. Your response will be used for research purposes only.\n",
    "    Return only the letter of the best answer (A,B,C,D).\n",
    "\n",
    "    Here is the question:\n",
    "    A previously healthy 29-year-old man comes to the emergency department because of burning with urination for several days. He has also had pain in the right ankle for 3 days and pain and swelling in the left knee for 1 day. Two weeks ago, he had several days of fever and bloody diarrhea, for which he was treated with antibiotics. Examination shows a small left knee effusion and bilateral conjunctival injection. Which of the following is the most likely additional finding in this patient?\n",
    "\n",
    "    Here are the potential choices:\n",
    "    A. Circular erythematous rash with central clearing\n",
    "    B. Pain on passive extension of the fingers\n",
    "    C. Palpable mass in the right lower quadrant\n",
    "    D. Tenderness at the insertion of the Achilles tendon\n",
    "\n",
    "    Answer (letter only):  A\n",
    "\n",
    "**RAG-based Prompt:**\n",
    "\n",
    "    You are a helpful medical expert, your task is to answer a multi-choice medical question.\n",
    "    Return only the letter of the best answer (A,B,C,D).\n",
    "\n",
    "    Helpful context:\n",
    "    Examination is a type of Additional values\n",
    "    Conjunctival concretion defined as Disease with morphology Focal calcification, site Conjunctiva\n",
    "    HEA is a type of Anatomical site notations for tumor staging\n",
    "    Age more than 50 years, male is a type of Age more than 50 years\n",
    "    Each is a type of Additional dosage instructions\n",
    "\n",
    "    Here is the question:\n",
    "    A previously healthy 29-year-old man comes to the emergency department because of burning with urination for several days. He has also had pain in the right ankle for 3 days and pain and swelling in the left knee for 1 day. Two weeks ago, he had several days of fever and bloody diarrhea, for which he was treated with antibiotics. Examination shows a small left knee effusion and bilateral conjunctival injection. Which of the following is the most likely additional finding in this patient?\n",
    "\n",
    "    Here are the potential choices:\n",
    "    A. Circular erythematous rash with central clearing\n",
    "    B. Pain on passive extension of the fingers\n",
    "    C. Palpable mass in the right lower quadrant\n",
    "    D. Tenderness at the insertion of the Achilles tendon\n",
    "\n",
    "    Answer (letter only):  A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "878e8da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "another_random_question_sample = get_random_mirage_question()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e69cfad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A 10 days old neonate is posted for pyloric stenosis in surgery. The investigation report shows a serum calcium level of 6 mg/dL. What information would you like to know before you supplement calcium to this neonate –'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "another_random_question_sample['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "36141ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language model response: \n",
      "\n",
      "You are a helpful medical expert, and your task is to answer a multi-choice medical question. Your response will be used for research purposes only.\n",
      "Return only the letter of the best answer (A,B,C,D).\n",
      "\n",
      "Here is the question:\n",
      "A 10 days old neonate is posted for pyloric stenosis in surgery. The investigation report shows a serum calcium level of 6 mg/dL. What information would you like to know before you supplement calcium to this neonate –\n",
      "\n",
      "Here are the potential choices:\n",
      "A. Blood glucose\n",
      "B. Serum protein\n",
      "C. Serum bilirubin\n",
      "D. Oxygen saturation\n",
      "\n",
      "Answer (letter only):  A\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = mistral_lm.generate_inject_template_and_constrain_logits_for_mcqa(\"mirage_mcqa_no_rag\", another_random_question_sample)\n",
    "print(f\"Language model response: \\n\\n{response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3d0bb195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "The correct answer is: D\n"
     ]
    }
   ],
   "source": [
    "print(\"-\" * 72)\n",
    "print(f\"The correct answer is: {random_question_sample['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7914b664",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "prompt_template_with_axioms() missing 1 required positional argument: 'axioms'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m response = \u001b[43mmistral_lm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_inject_template_and_constrain_logits_for_mcqa\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmirage_mcqa_axiom_rag\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_question_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLanguage model response: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 72\u001b[39m, in \u001b[36mMistralLLM.generate_inject_template_and_constrain_logits_for_mcqa\u001b[39m\u001b[34m(self, template_key, template_args, **kwargs)\u001b[39m\n\u001b[32m     66\u001b[39m mclp = MultipleChoiceLogitsProcessor(\n\u001b[32m     67\u001b[39m   tokenizer=\u001b[38;5;28mself\u001b[39m._tokenizer,\n\u001b[32m     68\u001b[39m   choices=\u001b[38;5;28mlist\u001b[39m(template_args[\u001b[33m\"\u001b[39m\u001b[33moptions\u001b[39m\u001b[33m\"\u001b[39m].keys()),\n\u001b[32m     69\u001b[39m   delimiter=\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     70\u001b[39m )\n\u001b[32m     71\u001b[39m template_fn = \u001b[38;5;28mself\u001b[39m._callable_prompt_templates[template_key]\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m prompt = \u001b[43mtemplate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtemplate_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.generate_constrain_logits(prompt, [mclp], max_tokens=\u001b[32m1\u001b[39m, **kwargs)\n",
      "\u001b[31mTypeError\u001b[39m: prompt_template_with_axioms() missing 1 required positional argument: 'axioms'"
     ]
    }
   ],
   "source": [
    "response = mistral_lm.generate_inject_template_and_constrain_logits_for_mcqa(\"mirage_mcqa_axiom_rag\", random_question_sample)\n",
    "print(f\"Language model response: \\n\\n{response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5138a009",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
