{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c3f89ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using RAG: False\n",
      "Processing pubmedqa ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 1533.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pubmedqa ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:26<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correct:   204\n",
      "Total incorrect: 296\n",
      "Accuracy:        40.8%\n",
      "\n",
      "\n",
      "Using RAG: False\n",
      "Processing pubmedqa ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 618.76it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pubmedqa ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:25<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correct:   216\n",
      "Total incorrect: 284\n",
      "Accuracy:        43.2%\n",
      "\n",
      "\n",
      "Using RAG: True\n",
      "Processing pubmedqa ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 1530.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pubmedqa ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:44<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correct:   226\n",
      "Total incorrect: 274\n",
      "Accuracy:        45.2%\n",
      "\n",
      "\n",
      "Using RAG: True\n",
      "Processing pubmedqa ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 809.01it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pubmedqa ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:35<00:00,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correct:   212\n",
      "Total incorrect: 288\n",
      "Accuracy:        42.4%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from retrievers import (\n",
    "  BaseRetriever,\n",
    "  SBERTRetriever,\n",
    "  HiTRetriever,\n",
    "  OnTRetriever\n",
    ")\n",
    "from pathlib import Path\n",
    "from math_functools import (\n",
    "  batch_cosine_similarity,\n",
    "  batch_poincare_dist_with_adaptive_curv_k,\n",
    "  entity_subsumption,\n",
    "  concept_subsumption\n",
    ")\n",
    "from llm_utils_improvements import (\n",
    "  MistralLLM,\n",
    "  BaseEntitySelector,\n",
    "  SimilarityEntitySelector,\n",
    "  ApproximateNearestNeighbourEntitySelector,\n",
    "  SubsumptionEntitySelector,\n",
    "  chat_prompt_template_no_rag,\n",
    "  chat_prompt_template_with_axioms\n",
    ")\n",
    "from harness_utils import (\n",
    "  QATestHarness\n",
    ")\n",
    "\n",
    "# ------------------------------------\n",
    "# LLM options:\n",
    "# ------------------------------------\n",
    "# \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "# \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "# \"BioMistral/BioMistral-7B\"\n",
    "# ------------------------------------\n",
    "\n",
    "LLM_MODEL_ID = \"BioMistral/BioMistral-7B\"\n",
    "SEED = 42\n",
    "\n",
    "# instanciate a retriever\n",
    "sbert_ret = SBERTRetriever(\n",
    "  embeddings_fp=Path(f\"./embeddings/sbert-plm-embeddings.npy\"),\n",
    "  meta_map_fp=Path(\"./embeddings/axiom-mappings.json\"),\n",
    "  verbalisations_fp=Path(\"./embeddings/axiom-verbalisations.json\"),\n",
    "  model_str=\"all-MiniLM-L12-v2\",\n",
    "  score_fn=batch_cosine_similarity\n",
    ")\n",
    "\n",
    "# and an entity selector\n",
    "sbert_entity_selector = SimilarityEntitySelector(sbert_ret)\n",
    "\n",
    "# and an LLM\n",
    "mistral_llm = MistralLLM(LLM_MODEL_ID)\n",
    "\n",
    "mistral_llm.load_tokenizer(use_fast=True).load_model(\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ").register_generation_config(\n",
    "    do_sample=False,\n",
    "    num_beams=1,\n",
    "    pad_token_id=mistral_llm._tokenizer.pad_token_id,\n",
    "    eos_token_id=mistral_llm._tokenizer.eos_token_id\n",
    ")\n",
    "mistral_llm.register_prompt_template_fn(\"mirage_mcqa_no_rag_chat\", chat_prompt_template_no_rag)\n",
    "mistral_llm.register_prompt_template_fn(\"mirage_mcqa_axiom_rag_chat\", chat_prompt_template_with_axioms)\n",
    "\n",
    "# ideally, we would load from config (TODO: load cfgNode \\w yacs or hydra)\n",
    "tests = QATestHarness(\n",
    "  Path(\"./data/MIRAGE/benchmark.json\"), \n",
    "  Path(\"./data/MIRAGE/benchmark-questions-entities-BIOMED-bionlp13cg.json\"), \n",
    "  Path(\"./data/MIRAGE/benchmark-questions-entities-HEAD.json\")\n",
    ").set_shuffle_question_options(True).set_permute_question_options(\n",
    "  True\n",
    ").set_retrieval_k(100).set_append_k(10).set_top_k(1).set_use_rag(True).register_retriever(\n",
    "  sbert_ret\n",
    ").register_entity_selector(\n",
    "  sbert_entity_selector\n",
    ").register_llm(\n",
    "  mistral_llm\n",
    ")\n",
    "\n",
    "# quick tests:\n",
    "\n",
    "QATestHarness.set_random_seed(SEED)\n",
    "\n",
    "tests.set_use_rag(False)\n",
    "tests.run_multiple(['pubmedqa'])\n",
    "\n",
    "tests.set_use_rag(False)\n",
    "tests.run_multiple(['pubmedqa'])\n",
    "\n",
    "# quick test to compare results to:\n",
    "\n",
    "tests.set_use_rag(True)\n",
    "tests.run_multiple(['pubmedqa'])\n",
    "\n",
    "tests.set_use_rag(True)\n",
    "tests.run_multiple(['pubmedqa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e4a8b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using RAG: True\n",
      "Processing pubmedqa ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 1030.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pubmedqa ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]/home/jon/ont-rag/scripts/math_functools.py:110: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /home/conda/feedstock_root/build_artifacts/libtorch_1750199048837/work/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
      "  vs_t = torch.as_tensor(vs_chunk, device=device, dtype=dtype)\n",
      "100%|██████████| 500/500 [03:07<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correct: 232\n",
      "Total incorrect: 268\n",
      "Accuracy: 46.4% \n",
      "\n",
      "Using RAG: True\n",
      "Processing bioasq ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 618/618 [00:00<00:00, 1458.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing bioasq ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 618/618 [03:48<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correct: 366\n",
      "Total incorrect: 252\n",
      "Accuracy: 59.22% \n",
      "\n",
      "Using RAG: True\n",
      "Processing mmlu ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1089/1089 [00:00<00:00, 1268.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing mmlu ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1089/1089 [11:21<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correct: 544\n",
      "Total incorrect: 545\n",
      "Accuracy: 49.95% \n",
      "\n",
      "Using RAG: True\n",
      "Processing medqa ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1273/1273 [00:00<00:00, 17134.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing medqa ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1273/1273 [30:43<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correct: 515\n",
      "Total incorrect: 758\n",
      "Accuracy: 40.46% \n",
      "\n",
      "Using RAG: True\n",
      "Processing medmcqa ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4183/4183 [00:02<00:00, 1600.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing medmcqa ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4183/4183 [25:58<00:00,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correct: 1485\n",
      "Total incorrect: 2698\n",
      "Accuracy: 35.5% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from retrievers import (\n",
    "  BaseRetriever,\n",
    "  SBERTRetriever,\n",
    "  HiTRetriever,\n",
    "  OnTRetriever\n",
    ")\n",
    "from pathlib import Path\n",
    "from math_functools import (\n",
    "  batch_cosine_similarity,\n",
    "  batch_poincare_dist_with_adaptive_curv_k,\n",
    "  batch_poincare_dist_with_adaptive_curv_k_torch,\n",
    "  entity_subsumption,\n",
    "  concept_subsumption\n",
    ")\n",
    "from llm_utils import (\n",
    "  MistralLLM,\n",
    "  BaseEntitySelector,\n",
    "  SimilarityEntitySelector,\n",
    "  ApproximateNearestNeighbourEntitySelector,\n",
    "  SubsumptionEntitySelector,\n",
    "  chat_prompt_template_no_rag,\n",
    "  chat_prompt_template_with_axioms\n",
    ")\n",
    "from harness_utils import (\n",
    "  QATestHarness\n",
    ")\n",
    "\n",
    "##########\n",
    "# GLOBALS\n",
    "##########\n",
    "\n",
    "# ------------------------------------\n",
    "# LLM options:\n",
    "# ------------------------------------\n",
    "# \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "# \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "# \"BioMistral/BioMistral-7B\"\n",
    "# ------------------------------------\n",
    "\n",
    "LLM_MODEL_ID = \"BioMistral/BioMistral-7B\"\n",
    "SEED = 42\n",
    "\n",
    "##################################################\n",
    "# BOOTSTRAP: encoder, retriever & entity selector\n",
    "##################################################\n",
    "\n",
    "common_map = Path(\"./embeddings/axiom-mappings.json\") # *entity mappings\n",
    "common_verbalisations = Path(\"./embeddings/axiom-verbalisations.json\") # rdfs:label(s) & verbs\n",
    "embeddings_dir = \"./embeddings\" # dir for embeddings\n",
    "\n",
    "# fine-tuned embedding model, for embedding entity mentions\n",
    "retriever_model_fp = hit_SNOMED25_model_path = Path('./models/HiT-mixed-SNOMED-25/final')\n",
    "\n",
    "# accepts an entity mention &\n",
    "# 1. produces an embedding\n",
    "# 2. measures the `score_fn` agaisnt existing embeddings\n",
    "# 3. returns a ranked list of entities as tuples: (rank, iri, score, verbalisation)\n",
    "hit_retriever = HiTRetriever(\n",
    "  embeddings_fp=Path(f\"{embeddings_dir}/hit-snomed-25-embeddings.npy\"),\n",
    "  meta_map_fp=common_map,\n",
    "  verbalisations_fp=common_verbalisations,\n",
    "  model_fp=retriever_model_fp,\n",
    "  score_fn=batch_poincare_dist_with_adaptive_curv_k_torch\n",
    ")\n",
    "\n",
    "# provides a ranking for a pool of entities (drawn from multiple multiple mentions for the same question)\n",
    "entity_selector = ApproximateNearestNeighbourEntitySelector(hit_retriever)\n",
    "\n",
    "##################\n",
    "#  BOOTSTRAP: LLM\n",
    "##################\n",
    "\n",
    "# initialises a LLM & exposes methods for \n",
    "# RAG \\w axiom verbalisation-based prompt enrichment\n",
    "mistral_llm = MistralLLM(LLM_MODEL_ID)\n",
    "\n",
    "mistral_llm.load_tokenizer(use_fast=True).load_model(\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ").register_generation_config(\n",
    "    do_sample=False,\n",
    "    num_beams=1,\n",
    "    pad_token_id=mistral_llm._tokenizer.pad_token_id,\n",
    "    eos_token_id=mistral_llm._tokenizer.eos_token_id\n",
    ")\n",
    "mistral_llm.register_prompt_template_fn(\"mirage_mcqa_no_rag_chat\", chat_prompt_template_no_rag)\n",
    "mistral_llm.register_prompt_template_fn(\"mirage_mcqa_axiom_rag_chat\", chat_prompt_template_with_axioms)\n",
    "\n",
    "# ideally, we would load from config (TODO: load cfgNode \\w yacs or hydra)\n",
    "tests = QATestHarness(\n",
    "  Path(\"./data/MIRAGE/benchmark.json\"), \n",
    "  Path(\"./data/MIRAGE/benchmark-questions-entities-BIOMED-bionlp13cg.json\"), \n",
    "  Path(\"./data/MIRAGE/benchmark-questions-entities-HEAD.json\")\n",
    ").set_shuffle_question_options(True).set_permute_question_options(\n",
    "  True\n",
    ").set_retrieval_k(100).set_append_k(10).set_top_k(1).set_use_rag(True).register_retriever(\n",
    "  hit_retriever\n",
    ").register_entity_selector(\n",
    "  entity_selector # type: ignore\n",
    ").register_llm(\n",
    "  mistral_llm # type: ignore\n",
    ")\n",
    "\n",
    "# single test run:\n",
    "\n",
    "QATestHarness.set_random_seed(SEED)\n",
    "\n",
    "tests.set_use_rag(True)\n",
    "tests.run_multiple(['pubmedqa', 'bioasq', 'mmlu', 'medqa', 'medmcqa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a2721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrievers import (\n",
    "  BaseRetriever,\n",
    "  SBERTRetriever,\n",
    "  HiTRetriever,\n",
    "  OnTRetriever\n",
    ")\n",
    "from pathlib import Path\n",
    "from math_functools import (\n",
    "  batch_cosine_similarity,\n",
    "  batch_poincare_dist_with_adaptive_curv_k,\n",
    "  batch_poincare_dist_with_adaptive_curv_k_torch,\n",
    "  efficient_batch_poincare_distance_with_curv_k,\n",
    "  entity_subsumption,\n",
    "  concept_subsumption\n",
    ")\n",
    "from llm_utils import (\n",
    "  MistralLLM,\n",
    "  BaseEntitySelector,\n",
    "  SimilarityEntitySelector,\n",
    "  ApproximateNearestNeighbourEntitySelector,\n",
    "  SubsumptionEntitySelector,\n",
    "  chat_prompt_template_no_rag,\n",
    "  chat_prompt_template_with_axioms\n",
    ")\n",
    "from harness_utils import (\n",
    "  QATestHarness\n",
    ")\n",
    "\n",
    "##########\n",
    "# GLOBALS\n",
    "##########\n",
    "\n",
    "# ------------------------------------\n",
    "# LLM options:\n",
    "# ------------------------------------\n",
    "# \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "# \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "# \"BioMistral/BioMistral-7B\"\n",
    "# ------------------------------------\n",
    "\n",
    "LLM_MODEL_ID = \"BioMistral/BioMistral-7B\"\n",
    "SEED = 42\n",
    "\n",
    "##################################################\n",
    "# BOOTSTRAP: encoder, retriever & entity selector\n",
    "##################################################\n",
    "\n",
    "common_map = Path(\"./embeddings/axiom-mappings.json\") # *entity mappings\n",
    "common_verbalisations = Path(\"./embeddings/axiom-verbalisations.json\") # rdfs:label(s) & verbs\n",
    "embeddings_dir = \"./embeddings\" # dir for embeddings\n",
    "\n",
    "# fine-tuned embedding model, for embedding entity mentions\n",
    "retriever_model_fp = hit_SNOMED25_model_path = Path('./models/HiT-mixed-SNOMED-25/final')\n",
    "\n",
    "# accepts an entity mention &\n",
    "# 1. produces an embedding\n",
    "# 2. measures the `score_fn` agaisnt existing embeddings\n",
    "# 3. returns a ranked list of entities as tuples: (rank, iri, score, verbalisation)\n",
    "hit_retriever = HiTRetriever(\n",
    "  embeddings_fp=Path(f\"{embeddings_dir}/hit-snomed-25-embeddings.npy\"),\n",
    "  meta_map_fp=common_map,\n",
    "  verbalisations_fp=common_verbalisations,\n",
    "  model_fp=retriever_model_fp,\n",
    "  score_fn=efficient_batch_poincare_distance_with_curv_k\n",
    ")\n",
    "\n",
    "# provides a ranking for a pool of entities (drawn from multiple multiple mentions for the same question)\n",
    "entity_selector = ApproximateNearestNeighbourEntitySelector(hit_retriever)\n",
    "\n",
    "##################\n",
    "#  BOOTSTRAP: LLM\n",
    "##################\n",
    "\n",
    "# initialises a LLM & exposes methods for \n",
    "# RAG \\w axiom verbalisation-based prompt enrichment\n",
    "mistral_llm = MistralLLM(LLM_MODEL_ID)\n",
    "\n",
    "mistral_llm.load_tokenizer(use_fast=True).load_model(\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ").register_generation_config(\n",
    "    do_sample=False,\n",
    "    num_beams=1,\n",
    "    pad_token_id=mistral_llm._tokenizer.pad_token_id,\n",
    "    eos_token_id=mistral_llm._tokenizer.eos_token_id\n",
    ")\n",
    "mistral_llm.register_prompt_template_fn(\"mirage_mcqa_no_rag_chat\", chat_prompt_template_no_rag)\n",
    "mistral_llm.register_prompt_template_fn(\"mirage_mcqa_axiom_rag_chat\", chat_prompt_template_with_axioms)\n",
    "\n",
    "# ideally, we would load from config (TODO: load cfgNode \\w yacs or hydra)\n",
    "tests = QATestHarness(\n",
    "  Path(\"./data/MIRAGE/benchmark.json\"), \n",
    "  Path(\"./data/MIRAGE/benchmark-questions-entities-BIOMED-bionlp13cg.json\"), \n",
    "  Path(\"./data/MIRAGE/benchmark-questions-entities-HEAD.json\")\n",
    ").set_shuffle_question_options(True).set_permute_question_options(\n",
    "  True\n",
    ").set_retrieval_k(100).set_append_k(10).set_top_k(1).set_use_rag(True).register_retriever(\n",
    "  hit_retriever\n",
    ").register_entity_selector(\n",
    "  entity_selector # type: ignore\n",
    ").register_llm(\n",
    "  mistral_llm # type: ignore\n",
    ")\n",
    "\n",
    "# single test run:\n",
    "\n",
    "QATestHarness.set_random_seed(SEED)\n",
    "\n",
    "tests.set_use_rag(True)\n",
    "tests.run_multiple(['pubmedqa', 'bioasq', 'mmlu', 'medqa', 'medmcqa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a8891c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrievers import (\n",
    "  BaseRetriever,\n",
    "  SBERTRetriever,\n",
    "  HiTRetriever,\n",
    "  OnTRetriever\n",
    ")\n",
    "from pathlib import Path\n",
    "from math_functools import (\n",
    "  batch_cosine_similarity,\n",
    "  batch_poincare_dist_with_adaptive_curv_k,\n",
    "  batch_poincare_dist_with_adaptive_curv_k_torch,\n",
    "  entity_subsumption,\n",
    "  concept_subsumption\n",
    ")\n",
    "from llm_utils import (\n",
    "  MistralLLM,\n",
    "  BaseEntitySelector,\n",
    "  SimilarityEntitySelector,\n",
    "  ApproximateNearestNeighbourEntitySelector,\n",
    "  SubsumptionEntitySelector,\n",
    "  chat_prompt_template_no_rag,\n",
    "  chat_prompt_template_with_axioms\n",
    ")\n",
    "from harness_utils import (\n",
    "  QATestHarness\n",
    ")\n",
    "\n",
    "##########\n",
    "# GLOBALS\n",
    "##########\n",
    "\n",
    "# ------------------------------------\n",
    "# LLM options:\n",
    "# ------------------------------------\n",
    "# \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "# \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "# \"BioMistral/BioMistral-7B\"\n",
    "# ------------------------------------\n",
    "\n",
    "LLM_MODEL_ID = \"BioMistral/BioMistral-7B\"\n",
    "SEED = 42\n",
    "\n",
    "##################################################\n",
    "# BOOTSTRAP: encoder, retriever & entity selector\n",
    "##################################################\n",
    "\n",
    "common_map = Path(\"./embeddings/axiom-mappings.json\") # *entity mappings\n",
    "common_verbalisations = Path(\"./embeddings/axiom-verbalisations.json\") # rdfs:label(s) & verbs\n",
    "embeddings_dir = \"./embeddings\" # dir for embeddings\n",
    "\n",
    "# fine-tuned embedding model, for embedding entity mentions\n",
    "retriever_model_fp = hit_SNOMED25_model_path = Path('./models/HiT-mixed-SNOMED-25/final')\n",
    "\n",
    "# accepts an entity mention &\n",
    "# 1. produces an embedding\n",
    "# 2. measures the `score_fn` agaisnt existing embeddings\n",
    "# 3. returns a ranked list of entities as tuples: (rank, iri, score, verbalisation)\n",
    "hit_retriever = HiTRetriever(\n",
    "  embeddings_fp=Path(f\"{embeddings_dir}/hit-snomed-25-embeddings.npy\"),\n",
    "  meta_map_fp=common_map,\n",
    "  verbalisations_fp=common_verbalisations,\n",
    "  model_fp=retriever_model_fp,\n",
    "  score_fn=batch_poincare_dist_with_adaptive_curv_k_torch\n",
    ")\n",
    "\n",
    "# provides a ranking for a pool of entities (drawn from multiple multiple mentions for the same question)\n",
    "entity_selector = ApproximateNearestNeighbourEntitySelector(hit_retriever)\n",
    "\n",
    "##################\n",
    "#  BOOTSTRAP: LLM\n",
    "##################\n",
    "\n",
    "# initialises a LLM & exposes methods for \n",
    "# RAG \\w axiom verbalisation-based prompt enrichment\n",
    "mistral_llm = MistralLLM(LLM_MODEL_ID)\n",
    "\n",
    "mistral_llm.load_tokenizer(use_fast=True).load_model(\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ").register_generation_config(\n",
    "    do_sample=False,\n",
    "    num_beams=1,\n",
    "    pad_token_id=mistral_llm._tokenizer.pad_token_id,\n",
    "    eos_token_id=mistral_llm._tokenizer.eos_token_id\n",
    ")\n",
    "mistral_llm.register_prompt_template_fn(\"mirage_mcqa_no_rag_chat\", chat_prompt_template_no_rag)\n",
    "mistral_llm.register_prompt_template_fn(\"mirage_mcqa_axiom_rag_chat\", chat_prompt_template_with_axioms)\n",
    "\n",
    "# ideally, we would load from config (TODO: load cfgNode \\w yacs or hydra)\n",
    "tests = QATestHarness(\n",
    "  Path(\"./data/MIRAGE/benchmark.json\"), \n",
    "  Path(\"./data/MIRAGE/benchmark-questions-entities-BIOMED-bionlp13cg.json\"), \n",
    "  Path(\"./data/MIRAGE/benchmark-questions-entities-HEAD.json\")\n",
    ").set_shuffle_question_options(True).set_permute_question_options(\n",
    "  True\n",
    ").set_retrieval_k(100).set_append_k(10).set_top_k(1).set_use_rag(True).register_retriever(\n",
    "  hit_retriever\n",
    ").register_entity_selector(\n",
    "  entity_selector # type: ignore\n",
    ").register_llm(\n",
    "  mistral_llm # type: ignore\n",
    ")\n",
    "\n",
    "# single test run:\n",
    "\n",
    "QATestHarness.set_random_seed(SEED)\n",
    "\n",
    "tests.set_use_rag(True)\n",
    "tests.run_multiple(['pubmedqa', 'bioasq', 'mmlu', 'medqa', 'medmcqa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d706b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using RAG: True\n",
      "Processing pubmedqa ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 1411.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pubmedqa ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 151\u001b[39m\n\u001b[32m    148\u001b[39m QATestHarness.set_random_seed(SEED)\n\u001b[32m    150\u001b[39m tests.set_use_rag(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m \u001b[43mtests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_multiple\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpubmedqa\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbioasq\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmmlu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmedqa\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmedmcqa\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ont-rag/scripts/harness_utils.py:390\u001b[39m, in \u001b[36mQATestHarness.run_multiple\u001b[39m\u001b[34m(self, datasets)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"run tests for a subset of the datasets associated with the provided benchmark\"\"\"\u001b[39;00m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ont-rag/scripts/harness_utils.py:315\u001b[39m, in \u001b[36mQATestHarness.run\u001b[39m\u001b[34m(self, dataset_name)\u001b[39m\n\u001b[32m    312\u001b[39m   question[\u001b[33m'\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m'\u001b[39m] = new_answer_key\n\u001b[32m    314\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._use_rag:\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_entity_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_and_rank_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mentities\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mret_k\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retrieval_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mappend_k_per_entity\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_append_k\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    316\u001b[39m   top_candidate = \u001b[38;5;28mself\u001b[39m._entity_selector._all_mention_results[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._entity_selector._all_mention_results) > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[32m    317\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m top_candidate:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ont-rag/scripts/llm_utils.py:386\u001b[39m, in \u001b[36mApproximateNearestNeighbourEntitySelector.encode_and_rank_candidates\u001b[39m\u001b[34m(self, entities, ret_k, append_k_per_entity, reverse_score_order, **kwargs)\u001b[39m\n\u001b[32m    384\u001b[39m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    385\u001b[39m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retriever\u001b[49m\u001b[43m.\u001b[49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m  \u001b[49m\u001b[43mmention\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mentity_literal\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m  \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mret_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m  \u001b[49m\u001b[43mreverse_candidate_scores\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreverse_score_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retriever\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m    391\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m results:\n\u001b[32m    393\u001b[39m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ont-rag/scripts/retrievers.py:258\u001b[39m, in \u001b[36mBaseModelRetriever.retrieve\u001b[39m\u001b[34m(self, query_string, top_k, reverse_candidate_scores, **kwargs)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mretrieve\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_string: \u001b[38;5;28mstr\u001b[39m, *, top_k: \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m, reverse_candidate_scores=\u001b[38;5;28;01mFalse\u001b[39;00m, **kwargs) -> \u001b[38;5;28mlist\u001b[39m[QueryResult]:\n\u001b[32m    257\u001b[39m       q = \u001b[38;5;28mself\u001b[39m._embed(query_string)                             \u001b[38;5;66;03m# np.float32\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m       scores = \u001b[38;5;28mself\u001b[39m._score_fn(q, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bank\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embeddings\u001b[49m, **kwargs)\n\u001b[32m    260\u001b[39m       \u001b[38;5;66;03m# scores is xp array (np/cp); argpartition is ~O(N) vs full sort O(N log N)\u001b[39;00m\n\u001b[32m    261\u001b[39m       xp = \u001b[38;5;28mself\u001b[39m._xp\n",
      "\u001b[31mValueError\u001b[39m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "from retrievers import (\n",
    "  BaseRetriever,\n",
    "  SBERTRetriever,\n",
    "  HiTRetriever,\n",
    "  OnTRetriever\n",
    ")\n",
    "from pathlib import Path\n",
    "from math_functools import (\n",
    "  batch_cosine_similarity,\n",
    "  batch_poincare_dist_with_adaptive_curv_k,\n",
    "  batch_poincare_dist_with_adaptive_curv_k_torch,\n",
    "  entity_subsumption,\n",
    "  concept_subsumption\n",
    ")\n",
    "from llm_utils import (\n",
    "  MistralLLM,\n",
    "  BaseEntitySelector,\n",
    "  SimilarityEntitySelector,\n",
    "  ApproximateNearestNeighbourEntitySelector,\n",
    "  SubsumptionEntitySelector,\n",
    "  chat_prompt_template_no_rag,\n",
    "  chat_prompt_template_with_axioms\n",
    ")\n",
    "from harness_utils import (\n",
    "  QATestHarness\n",
    ")\n",
    "import torch\n",
    "\n",
    "##########\n",
    "# GLOBALS\n",
    "##########\n",
    "\n",
    "# ------------------------------------\n",
    "# LLM options:\n",
    "# ------------------------------------\n",
    "# \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "# \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "# \"BioMistral/BioMistral-7B\"\n",
    "# ------------------------------------\n",
    "\n",
    "LLM_MODEL_ID = \"BioMistral/BioMistral-7B\"\n",
    "SEED = 42\n",
    "\n",
    "##################################################\n",
    "# BOOTSTRAP: encoder, retriever & entity selector\n",
    "##################################################\n",
    "\n",
    "common_map = Path(\"./embeddings/axiom-mappings.json\") # *entity mappings\n",
    "common_verbalisations = Path(\"./embeddings/axiom-verbalisations.json\") # rdfs:label(s) & verbs\n",
    "embeddings_dir = \"./embeddings\" # dir for embeddings\n",
    "\n",
    "retriever_model_fp = hit_SNOMED25_model_path = Path('./models/HiT-mixed-SNOMED-25/final')\n",
    "# self,\n",
    "#         verbalisations_fp: Path,\n",
    "#         meta_map_fp: Path,\n",
    "#         embeddings_fp: Path,\n",
    "#         *,\n",
    "#         score_fn: Callable | None = None,\n",
    "#         model_fp: Path | None = None,\n",
    "#         model_str: str | None = None,\n",
    "#         backend: Backend = \"numpy\",            # <— control CPU vs GPU\n",
    "#         resident: bool = True,                 # <— copy once & keep on device\n",
    "#         eps: float = 1e-7\n",
    "# Poincaré (HiT/OnT) on GPU Torch, resident bank:\n",
    "hit_retriever = HiTRetriever(\n",
    "    verbalisations_fp=common_verbalisations,\n",
    "    meta_map_fp=common_map,\n",
    "    embeddings_fp=Path(f\"{embeddings_dir}/hit-snomed-25-embeddings.npy\"),\n",
    "    model_fp=retriever_model_fp,\n",
    "    backend=\"cupy\",\n",
    "    resident=True\n",
    ")\n",
    "\n",
    "hit_retriever._set_curvature_from_model()\n",
    "\n",
    "# hit_retriever.register_local_model(retriever_model_fp)\n",
    "# hit_retriever._set_curvature_from_model()\n",
    "# print(hit_retriever._curv_k)\n",
    "\n",
    "# results = ret.retrieve(\"what is ...\", top_k=10)\n",
    "\n",
    "# # SBERT cosine on CPU NumPy:\n",
    "# sret = SBERTRetriever(\n",
    "#     Path(\"verbalisations.json\"),\n",
    "#     Path(\"meta_map.json\"),\n",
    "#     Path(\"sbert_embeddings.npy\"),\n",
    "#     backend=\"numpy\",\n",
    "# )\n",
    "# sret.use_cosine()\n",
    "# results2 = sret.retrieve(\"a query\", top_k=20, reverse_candidate_scores=True)  # if you want highest cosine\n",
    "\n",
    "# fine-tuned embedding model, for embedding entity mentions\n",
    "# retriever_model_fp = hit_SNOMED25_model_path = Path('./models/HiT-mixed-SNOMED-25/final')\n",
    "\n",
    "# accepts an entity mention &\n",
    "# 1. produces an embedding\n",
    "# 2. measures the `score_fn` agaisnt existing embeddings\n",
    "# 3. returns a ranked list of entities as tuples: (rank, iri, score, verbalisation)\n",
    "# hit_retriever = HiTRetriever(\n",
    "#   embeddings_fp=Path(f\"{embeddings_dir}/hit-snomed-25-embeddings.npy\"),\n",
    "#   meta_map_fp=common_map,\n",
    "#   verbalisations_fp=common_verbalisations,\n",
    "#   model_fp=retriever_model_fp,\n",
    "#   score_fn=batch_poincare_dist_with_adaptive_curv_k_torch\n",
    "# )\n",
    "\n",
    "# provides a ranking for a pool of entities (drawn from multiple multiple mentions for the same question)\n",
    "entity_selector = ApproximateNearestNeighbourEntitySelector(hit_retriever)\n",
    "\n",
    "##################\n",
    "#  BOOTSTRAP: LLM\n",
    "##################\n",
    "\n",
    "# initialises a LLM & exposes methods for \n",
    "# RAG \\w axiom verbalisation-based prompt enrichment\n",
    "mistral_llm = MistralLLM(LLM_MODEL_ID)\n",
    "\n",
    "mistral_llm.load_tokenizer(use_fast=True).load_model(\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ").register_generation_config(\n",
    "    do_sample=False,\n",
    "    num_beams=1,\n",
    "    pad_token_id=mistral_llm._tokenizer.pad_token_id,\n",
    "    eos_token_id=mistral_llm._tokenizer.eos_token_id\n",
    ")\n",
    "mistral_llm.register_prompt_template_fn(\"mirage_mcqa_no_rag_chat\", chat_prompt_template_no_rag)\n",
    "mistral_llm.register_prompt_template_fn(\"mirage_mcqa_axiom_rag_chat\", chat_prompt_template_with_axioms)\n",
    "\n",
    "# ideally, we would load from config (TODO: load cfgNode \\w yacs or hydra)\n",
    "tests = QATestHarness(\n",
    "  Path(\"./data/MIRAGE/benchmark.json\"), \n",
    "  Path(\"./data/MIRAGE/benchmark-questions-entities-BIOMED-bionlp13cg.json\"), \n",
    "  Path(\"./data/MIRAGE/benchmark-questions-entities-HEAD.json\")\n",
    ").set_shuffle_question_options(True).set_permute_question_options(\n",
    "  True\n",
    ").set_retrieval_k(100).set_append_k(10).set_top_k(1).set_use_rag(True).register_retriever(\n",
    "  hit_retriever\n",
    ").register_entity_selector(\n",
    "  entity_selector # type: ignore\n",
    ").register_llm(\n",
    "  mistral_llm # type: ignore\n",
    ")\n",
    "\n",
    "# single test run:\n",
    "\n",
    "QATestHarness.set_random_seed(SEED)\n",
    "\n",
    "tests.set_use_rag(True)\n",
    "tests.run_multiple(['pubmedqa', 'bioasq', 'mmlu', 'medqa', 'medmcqa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6987b388",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
